
https://blog.csdn.net/abcdocker/article/details/124855014
https://blog.csdn.net/zhaikaiyun/article/details/104633824/

https://blog.csdn.net/qq_25599925/article/details/123759160
https://blog.csdn.net/u012069313/article/details/125201017

https://blog.csdn.net/pleong/article/details/125086963
https://blog.csdn.net/clover661/article/details/120093243

=============git cli  安装================

yum  config-manager --add-repo https://cli.github.com/packages/rpm/gh-cli.repo
yum install gh


 
=====================================
阿里云 的k8s 网络设置
net.ipv4.ip_forward=1
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.[ethX].rp_filter = 0
net.ipv4.tcp_tw_reuse = 0
net.ipv4.tcp_tw_recycle = 0
net.ipv4.ip_local_port_range="32768 60999"
================================================

#基础安装资源
yum  install -y vim-enhanced  bash-completion  wget  curl  zip  bzip2  unzip \ 
               git  tcpdump lrzsz tree  sysstat  lsof man-pages  chrony

      tmux  -多窗口远程工具 lsof-查看进程打开的文件         
#编译安装资源
yum install  -y  iproute-tc    conntrack   ipvsadm ipset iptables net-tools pcre pcre-devel \
       openssl openssl-devel gcc  gcc-c++ glibc glibc-devel  make automake autoconf libtool  zlib-devel   

#宝塔要安装的包：
libcurl-devel wget tar zip unzip openssl openssl-devel gcc libxml2 libxml2-devel libxslt* zlib zlib-devel
 libjpeg-devel libpng-devel libwebp libwebp-devel freetype freetype-devel lsof pcre pcre-devel vixie-cron 
 crontabs icu libicu-devel c-ares libffi-devel bzip2-devel ncurses-devel sqlite-devel readline-devel tk-devel
 gdbm-devel db4-devel libpcap-devel xz-devel

==============阿里云源==  lvs+nginx+keepalived+k8s 高可用kubernetes（多master)==========================
1 关闭防火墙
systemctl stop firewalld
systemctl disable firewalld
iptables -F && iptables -X && iptables -F -t nat &&  iptables -X -t nat
iptables -P FORWARD ACCEPT
setenforce 0
sed -i  's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config

2. 关闭swap
swapoff -a 
sed  -i  '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

3. 同步服务器时间
timedatectl set-timezone Asia/Shanghai
yum install chrony -y 
sed -i "s/pool.*$/server ntp.aliyun.com iburst\nserver time1.cloud.tencent.com iburst\nserver ntp.tuna.tsinghua.edu.cn iburst/g" /etc/chrony.conf
systemctl enable chronyd 
systemctl start chronyd 
chronyc sources
hwclock -w

4.  确保唯一 uuid 和mac 地址
sudo cat /sys/class/dmi/id/product_uuid

5. hosts      dns名字统一设置到hosts中
cat >> /etc/hosts << EOF
192.168.110.20 master
192.168.110.25 node1
192.168.110.26 node2
EOF

6.  加载配置模块
lsmod | grep br_netfilter 来完成。若要显式加载该模块，可执行 sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe br_netfilter
sudo modprobe overlay

yum install iproute-tc -y  # 安装环境

7. 配置网络转发
cat  > /etc/sysctl.d/k8s.conf <<EOF
vm.swappiness = 0
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
user.max_user_namespaces=28633
EOF
sudo sysctl --system
# sysctl -p /etc/sysctl.d/k8s.conf

8. 系统参数更改
# 配置limit
ulimit -SHn 65535
cat >> /etc/security/limits.conf <<EOF
* soft nofile 655360
* hard nofile 131072
* soft nproc 655350
* hard nproc 655350
* seft memlock unlimited
* hard memlock unlimitedd
EOF

9. # ipvs 安装
yum install ipvsadm ipset sysstat conntrack libseccomp -y   #安装ipvs
#加载ipvs支持内核
cat >> /etc/modules-load.d/ipvs.conf <<EOF 
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF
systemctl restart systemd-modules-load.service
lsmod | grep -e ip_vs -e nf_conntrack

modprobe  br_netfilter

10. #修改内核参数
cat <<EOF > /etc/sysctl.d/k8s.conf
vm.swappiness = 0
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
user.max_user_namespaces=28633
fs.may_detach_mounts = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
net.ipv6.conf.all.disable_ipv6 = 0
net.ipv6.conf.default.disable_ipv6 = 0
net.ipv6.conf.lo.disable_ipv6 = 0
net.ipv6.conf.all.forwarding = 1
EOF
sysctl --system  #执行生效

11. #编译安装 nginx    #所有master节点安装
yum install -y pcre pcre-devel openssl openssl-devel gcc gcc-c++ automake autoconf libtool make  zlib-devel
wget https://nginx.org/download/nginx-1.20.2.tar.gz
tar xf nginx-1.20.2.tar.gz
cd nginx-1.20.2/
useradd nginx -s /sbin/nologin -M
./configure --prefix=/opt/nginx/ --with-pcre --with-http_ssl_module --with-http_stub_status_module --with-stream --with-http_stub_status_module --with-http_gzip_static_module
make  &&  make install  
cat >/usr/lib/systemd/system/nginx.service<<EOF
# /usr/lib/systemd/system/nginx.service
[Unit]
Description=The nginx HTTP and reverse proxy server
After=network.target sshd-keygen.service

[Service]
Type=forking
EnvironmentFile=/etc/sysconfig/sshd
ExecStartPre=/opt/nginx/sbin/nginx -t -c /opt/nginx/conf/nginx.conf
ExecStart=/opt/nginx/sbin/nginx -c /opt/nginx/conf/nginx.conf
ExecReload=/opt/nginx/sbin/nginx -s reload
ExecStop=/opt/nginx/sbin/nginx -s stop
Restart=on-failure
RestartSec=42s

[Install]
WantedBy=multi-user.target
EOF

#开机启动
[root@k8s-01 nginx-1.20.2]# systemctl enable nginx --now
  编译安装nginx的配置文件的路径： /opt/nignx/conf/nignx.conf

yum  install -y   nginx  #yum安装nginx需要注意下面的配置文件路径就是/etc/nginx/nginx.conf

vim  /opt/nginx/conf/nginx.conf  #配置nginx
  user nginx nginx;
  worker_processes auto;
  events {
      worker_connections  20240;
      use epoll;
  }
  error_log /var/log/nginx_error.log info;

  stream {
      upstream kube-servers {
          hash $remote_addr consistent;
          
          server rocky92:6443 weight=5 max_fails=1 fail_timeout=3s;  #这里可以写IP
          server 192.168.110.95:6443 weight=5 max_fails=1 fail_timeout=3s;
          server 192.168.110.96:6443 weight=5 max_fails=1 fail_timeout=3s;
      }

      server {
          listen 8443 reuseport;
          proxy_connect_timeout 3s;
          # 加大timeout
          proxy_timeout 3000s;
          proxy_pass kube-servers;
      }
  }
systemctl enable nginx --now     #启动nginx

13.  #安装keepalived   所有master安装
yum install -y keeplaived
#配置keepalived
cat > /etc/keepalived/keepalived.conf <<EOF
! Configuration File for keepalived
global_defs {
   router_id 192.168.110.92    #节点ip，master每个节点配置自己的IP
}
vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 8443"
    interval 2
    weight -20
}
vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 251
    priority 100
    advert_int 1
    mcast_src_ip 192.168.110.92    #节点IP
    nopreempt
    authentication {
        auth_type PASS
        auth_pass 11111111
    }
    track_script {
         chk_nginx
    }
    virtual_ipaddress {
        192.168.110.99   #VIP
    }
}
EOF
#编写健康检查脚本
vi /etc/keepalived/check_port.sh  
#!bin/bash
CHK_PORT=$1
 if [ -n "$CHK_PORT" ];then
        PORT_PROCESS=`ss -lnt|grep $CHK_PORT|wc -l`
        if [ $PORT_PROCESS -eq 0 ];then
                echo "Port $CHK_PORT Is Not Used,End."
                exit 1
        fi
 else
        echo "Check Port Cant Be Empty!"
 fi

#运行keeplived
systemctl  enable --now  keeplaived
#测试网络
ping  rocky99.cn   #VIP地址

14.  安装containerd
wget -O /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum install containerd -y

#配置 containerd
containerd config default > /etc/containerd/config.toml
sed -i "s#k8s.gcr.io/pause#registry.aliyuncs.com/google_containers/pause#g" /etc/containerd/config.toml
添加
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
            endpoint = ["https://9npjh5s8.mirror.aliyuncs.com"]
systemctl restart containerd
systemctl enable containerd
systemctl status containerd.service

cat<<END> install-2.sh 
#!/bin/bash
containerd config default > /etc/containerd/config.toml
sed -i "s#k8s.gcr.io/pause:3.6#registry.aliyuncs.com/google_containers/pause:3.7#g" /etc/containerd/config.toml
sed -i "s#systemd_cgroup = false#systemd_cgroup = true#g" /etc/containerd/config.toml

sed -i "s#https://registry-1.docker.io#https://0k0953tv.mirror.aliyuncs.com#g" /etc/containerd/config.toml
systemctl restart containerd
systemctl status containerd.service
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
END
sh  install-2.sh

15. 配置 crictl 环境
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF

16. 安装kubeadmin
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet

#设置crictl 运行环境
 crictl config runtime-endpoint unix:///run/containerd/containerd.sock  #自动生成、/etc/crictl.yaml 

17. 部署集群
mkdir -p /root/k8s-install
kubeadm  config print  init-defaults >  /root/k8s-install/kubeadm-ini.yaml  #生成配置文件

更改  kubeadm-init.yaml   -单master
sed -i "s/imageRepository.*$/imageRepository: registry.aliyuncs.com\/google_containers/g" /root/k8s-install/kubeadm-ini.yaml
sed -i "s/advertiseAddress: 1.2.3.4/advertiseAddress: 192.168.110.92/g"  /root/k8s-install/kubeadm-ini.yaml   # 设置apiserver的ip地址
sed -i "s/name: node/name: rocky92/g"  /root/k8s-install/kubeadm-ini.yaml    # 设置apiserver的hostname
在 serviceSubnet 下设置 podSubnet 172.16.0.0/16                               # 设置 pod网络地址段

多master ：
kind: ClusterConfiguration
kubernetesVersion: 1.24.0
controlPlaneEndpoint: apiserver.frps.cn:8443        #高可用地址，我这里填写vip
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 172.16.0.0/16              
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs      


下载镜像：
kubeadm  config images  list  --config /root/k8s-install/kubeadm-ini.yaml    #查看镜像
kubeadm  config  images pull  --config  /root/k8s-install/kubeadm-ini.yaml   #下载镜像

生成集群 #master执行
kubeadm init --config /root/k8s-install/kubeadm-ini.yaml --upload-certs |tee kubeadm-ini.log  #生成集群并保留日志

kubeadm join rocky99.cn:8443 --token abcdef.0123456789abcdef \   #多master加入集群
--discovery-token-ca-cert-hash sha256:22fccaea66461ff592bdcacc9acacf5357d53dafa68febab7b5621a3959dfd3a \
--control-plane --certificate-key 89bb8d8a4214c49ac81ee580fd682e2d8b9fcc52fddc7a45142167ecf7d0ed21

kubeadm join 192.168.110.92:6443 --token abcdef.0123456789abcdef \                      #node   执行加入集群
	--discovery-token-ca-cert-hash sha256:f5acc246eb029d2bcbcdf4f4d25d43d0f23764fd9556468f2e425cdf86fcc224 

配置命令环境
#master
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config       # master 执行
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#或设置环境变量
  export KUBECONFIG=/etc/kubernetes/admin.conf

# node                                                          
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/kubelet.conf  $HOME/.kube/config   #NODES 
sudo chown $(id -u):$(id -g) $HOME/.kube/config

18. # 安装calico 网络
cd /root/k8s-install
wget https://docs.projectcalico.org/manifests/calico.yaml
grep "image"  calico.yaml                                             #查看镜像
crictl pull  XXX                                                      #手动加载镜像
kubectl apply -f  calico.yaml                                         # 生成calico 网络ip设置

19.  # 验证k8s
kubectl -n kube-system get cm kubeadm-config -o yaml   #查看集群的配置
kubectl  get  cs    #查看集群
kubectl  get  pods  -A  -o wide    #查看pod  所有都running
kubectl create deployment nginx --image=nginx
kubectl  get  nodes    
kubectl expose deployment nginx --port=80 --type=NodePort
kubectl get pod,svc -o wide
输入访问地址：http://192.168.1.28:32039

20. #验证apiserver.crt证书有效期限
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep Not 
更新到10年有效期
git clone https://github.com/yuyicai/update-kube-cert.git
cd update-kubeadm-cert
chmod 755 update-kubeadm-cert.sh
sed -i "s/docker/crictl/g" update-kubeadm-cert.sh   #将docker 命令替换成crictl 命令
./update-kubeadm-cert.sh all                # 执行延期操作更改为10年

21. 生成加入 k8s 集群环境的加入token,默认 token 有效期为 24 小时，当过期需要重新创建 加入token，操作如下：
kubeadm token create  [--ttl=0]               # ttl=0 永不过期 
kubeadm token list                            #查看token
#下面是查询服务器token的hash256 
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
kubeadm token create --print-join-command   #直接生成join token

22. kubernetes 维护：
 kubeadm  reset  # 删除集群
 kubectl  delete nodes   xx    #删除node

23. kubernetes-Dashboard
  wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml
   编辑 recommended.yaml 更改service 的type为 NodePort
      kind: Service
      apiVersion: v1
      metadata:
        labels:
          k8s-app: kubernetes-dashboard
        name: kubernetes-dashboard
        namespace: kubernetes-dashboard
      spec:
        type: NodePort
        ports:
          - port: 443
            targetPort: 8443
            nodePort: 30001
        selector:
          k8s-app: kubernetes-dashboard
kubectl  apply -f recommended.yaml     #生成  dashboard 任务
访问  https://192.168.110.92:30001

  #token 方式登陆
  #wget https://raw.githubusercontent.com/cby-chen/Kubernetes/main/yaml/dashboard-user.yaml #实现如下功能
    kubectl  create serviceaccount  dashboard-admin -n kubernetes-dashboard    #生成账号
    kubectl get serviceaccount -n kubernetes-dashboard |grep  dashboard-admin  #查看账号
    kubectl create  clusterrolebinding dashboard-admin --clusterrole=cluster-admin \
              --serviceaccount=kubernetes-dashboard:dashboard-admin            #将账号绑定cluster-admin 角色
    kubectl get clusterrolebind -n kubernetes-dashboard |grep dashboard-admin

  kubectl create token -n kubernetes-dashboard  dashboard-admin    #生成并查看用token
  # kubeconfig登陆
  cd /etc/kubernetes/pki  #  k8s的master节点操作
  kubectl config set-cluster kubernetes --certificate-authority=./ca.crt --server="https://192.168.110.92:6443" \
  --embed-certs=true  --kubeconfig=/root/admin-user.conf    #创建 cluster
  DEF_NS_ADMIN_TOKEN=$(kubectl create token admin-user -n kubernetes-dashboard) #生成token并赋值
  kubectl config set-credentials admin-user --token=$DEF_NS_ADMIN_TOKEN  --kubeconfig=/root/admin-user.conf #开始创建credentials
  kubectl config set-context  admin-user@kubernetes --cluster=kubernetes --user=admin-user --kubeconfig=/root/admin-user.conf #创建context
  kubectl config use-context admin-user@kubernetes --kubeconfig=/root/admin-user.conf #切换context的current-context是admin-user@kubernetes #登陆选择kubeconfig 文件即可 
   
============================================= docker-ce  安装 =============================================
1.  安装
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
sudo sed -i 's+download.docker.com+mirrors.aliyun.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo
sudo yum makecache 
sudo yum -y install docker-ce                # 安装 docker-centos
sudo service docker start
2.  国内镜像加速
cat  > /etc/docker/daemon.json <<EOF
{
"registry-mirrors":  ["https://9npjh5s8.mirror.aliyuncs.com"],
"exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
3. 启动服务
systemctl daemon-reload
systemctl enable docker
systemctl restart docker
==========================================阿里云  k8s 学习实践 ======================
++++++【体验】K8S API 基础及Pod 基本应用++++
pad-demo.yaml:
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo
spec:
  containers:
  - name: demo
    image: ikubernetes/demoapp:v1.0
    imagePullPolicy: IfNotPresent
kubectl apply -f pod-demo.yaml  #生成pod
kubectl get pods/pod-demo -w    # 实时查看pod  ^c  停止
kubectl get pods/pod-demo -o yaml #yaml  格式查看
kubectl describe pods/pod-demo   #pod详情
podIP=$(kubectl get pods/pod-demo -o jsonpath={.status.podIP})   # 得到pod的 ip地址
#开启另一个pod 并执行shell进入，
kubectl run pod-$RANDOM --image="ikubernetes/admin-box:v1.2" --restart=Never --env=podIP=$podIP --rm -it --command -- /bin/sh 
root@pod-2545 # curl 172.16.193.137  #已经进入容器中的shell
iKubernetes demoapp v1.0 !! ClientIP: 172.16.165.12, ServerName: pod-demo, ServerIP: 172.16.193.137!
root@pod-2545 # curl 172.16.193.137/user-agent
User-Agent: curl/7.67.0
#查看容器的监控端口
 kubectl exec pods/pod-demo -c demo -it -- netstat -tnlp
+++++++++
cat >pod_with_cmd_args.yaml <<EOF
   # Created-By:"MageEdu <mage@magedu.com>"
   apiVersion: v1
   kind: Pod
   metadata:
     name: pod-demo-with-cmd-and-args
   spec:
     containers:
     - name: demo
       image: ikubernetes/demoapp:v1.0
       imagePullPolicy: IfNotPresent
       command: ['/bin/sh','-c']
       args: ['python3 /usr/local/bin/demo.py -p 8080']
   # Created-By:"MageEdu <mage@magedu.com>"
   apiVersion: v1
   kind: Pod
   metadata:
     name: pod-demo-with-cmd-and-args
   spec:
     containers:
     - name: demo
       image: ikubernetes/demoapp:v1.0
       imagePullPolicy: IfNotPresent
       command: ['/bin/sh','-c']
       args: ['python3 /usr/local/bin/demo.py -p 8080']
EOF

kubectl apply -f pod-demo-with-cmd-and-args.yaml # 生成pod
kubectl get pods/pod-demo-with-cmd-and-args   #查看pod
kubectl exec pod-demo-with-cmd-and-args -- netstat -tnl   #执行pod内shell的命令， 查看netstat 命令执行结果
++++
cat > pod_use_env.yaml <<EOF
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-env
spec:
  containers:
  - name: demo
    image: ikubernetes/demoapp:v1.0
    imagePullPolicy: IfNotPresent
    env:
    - name: HOST
      value: "127.0.0.1"
    - name: PORT
      value: "8080"
EOF
kubectl apply -f pod-using-env.yaml
kubectl get pods/pod-using-env
kubectl exec pod-using-env -- netstat -tnl
kubectl run pod pod-using-env-02 --image="ikubernetes/demoapp:v1.0" --restart=Never --env=HOST=127.0.0.1 --env=PORT=10080 -it --rm --command -- printenv | grep -e "^(HOST|PORT)"
kubectl get pods
kubectl delete pods --all --force --grace-period=0

++++多容器 Pod 及其设计模式+++
Step2：Sidecar容器
cat >sidecar-container-demo.yaml <<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-container-demo
spec:
  containers:
  - name: proxy   # 容器一
    image: envoyproxy/envoy-alpine:v1.16.2
    command: ['/bin/sh','-c']
    args: ['sleep 3 && envoy -c /etc/envoy/envoy.yaml']
    lifecycle:
      postStart:
        exec:
          command: ['/bin/sh','-c','wget -O /etc/envoy/envoy.yaml https://code.aliyun.com/MageEdu/kubernetes-ckad/raw/master/tutorials/handson/Understand_Multi-Container_Pod_design_patterns/envoy.yaml']
  - name: demoapp    # 容器二
    image: ikubernetes/demoapp:v1.0
    imagePullPolicy: IfNotPresent
    env:
    - name: HOST
      value: "127.0.0.1"
    - name: PORT
      value: "8080"
EOF
kubectl apply -f sidecar-container-demo.yaml
kubectl get pods -o wide
kubectl exec sidecar-container-demo -c proxy -- netstat -tnlp
envoyIP=$(kubectl get pods/sidecar-container-demo -o jsonpath={.status.podIP}); #查询容器podIP
kubectl run client-$RANDOM --image="ikubernetes/admin-toolbox:v1.0" --restart=Never --rm -it --env=envoyIP=${envoyIP} --command -- /bin/sh
    curl -I http://$envoyIP/
    exit
Step3：Ambassador容器
cat > ambassador-container-demo.yaml <<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: Pod
metadata:
  name: ambassador-container-demo
spec:
  containers:
  - name: curl
    image: ikubernetes/admin-toolbox:v1.0
    command: ["sleep", "999999"]
  - name: ambassador
    image: bitnami/kubectl:1.21
    command: ["/bin/sh","-c","kubectl proxy"]
    args:    # 传递给 kubectl proxy 的选项，若需要改变默认监听的tcp/8001端口，可以额外附加“--port=NUM”选项；
    - --server="https://kubernetes.default.svc"
    - --certificate-authority="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
    - --token="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"
    - --accept-paths='^.\*'
EOF

kubectl apply -f ambassador-container-demo.yaml
kubectl exec -it ambassador-container-demo -c curl -- /bin/sh
  curl localhost:8001/
  exit
+++++
Step4：Adapter容器
cat >adapter-container-demo.yaml<<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-conf
data:
  default.conf: |
    server {
      listen       80;
      server_name  localhost;
      location / {
          root   /usr/share/nginx/html;
          index  index.html index.htm;
      }
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   /usr/share/nginx/html;
      }
      location /nginx_status {
        stub_status;
        allow 127.0.0.1; 
        deny all;   
      }
    }
---
apiVersion: v1
kind: Pod
metadata:
  name: adapter-container-demo
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /etc/nginx/conf.d/
      name: nginx-conf
      readOnly: true
  - name: adapter
    image: nginx/nginx-prometheus-exporter:0.9.0
    args: ["-nginx.scrape-uri","http://localhost/nginx_status"]
    ports:      # nginx-prometheus-exporter默认监听tcp/9113端口
    - name: exporter
      containerPort: 9113   
  volumes:
  - name: nginx-conf
    configMap:
      name: nginx-conf
      items:
      - key: default.conf
        path: default.conf
EOF
kubectl apply -f adapter-container-demo.yaml
kubectl get pods/adapter-container-demo -o wide
adapterIP=$(kubectl get pods/adapter-container-demo -o jsonpath={.status.podIP});kubectl run client-$RANDOM --image="ikubernetes/admin-toolbox:v1.0"    --restart=Never --rm -it --env=adapterIP=${adapterIP} --command -- /bin/sh
    curl -I http://$adapterIP:9113/metrics/
    exit
++++++
Step6：Initializer容器
cat >init-container-demo.yaml<<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: Pod
metadata:
  name: init-container-demo
spec:
  initContainers:   # 定义初始化容器
  - name: iptables-init
    image: ikubernetes/admin-toolbox:v1.0
    imagePullPolicy: IfNotPresent
    command: ['/bin/sh','-c']
    args: ['iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80']
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
  containers:
  - name: demo
    image: ikubernetes/demoapp:v1.0
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
EOF
kubectl apply -f init-container-demo.yaml
kubectl describe pods/init-container-demo
 kubectl exec init-container-demo -- iptables -t nat -vnL
 podIP=$(kubectl get pods/init-container-demo -o jsonpath={.status.podIP});kubectl run client-$RANDOM --image="ikubernetes/admin-toolbox:v1.0" --restart=Never --rm -it --env=podIP=${podIP} --command -- /bin/sh
    curl http://$podIP:8080/
    exit

  =================================
  阿里云 存储卷与数据持久化
  ++++++Step2: emptyDir存储卷+++++++
cat  > volumes-emptydir-demo.yaml <<EOF
  # Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: volumes-emptydir-demo
spec:
  initContainers:
  - name: config-file-downloader
    image: ikubernetes/admin-box
    imagePullPolicy: IfNotPresent
    command: ['/bin/sh','-c','wget -O /data/envoy.yaml https://code.aliyun.com/MageEdu/kubernetes-start/raw/master/tutorials/handson/volume-basics-01/envoy.yaml']
    volumeMounts:
    - name: config-file-store
      mountPath: /data
  containers:
  - name: envoy
    image: envoyproxy/envoy-alpine:v1.16.2
    command: ['/bin/sh','-c']
    args: ['envoy -c /etc/envoy/envoy.yaml']
    volumeMounts:
    - name: config-file-store
      mountPath: /etc/envoy
      readOnly: true
  volumes:
  - name: config-file-store
    emptyDir:
      medium: Memory
      sizeLimit: 16Mi
EOF
kubectl apply -f volumes-emptydir-demo.yaml
kubectl describe pods volumes-emptydir-demo
kubectl exec volumes-emptydir-demo -- netstat -tnl
podIP=$(kubectl get pods/volumes-emptydir-demo -o jsonpath={.status.podIP});kubectl run pod-$RANDOM --image="ikubernetes/admin-box:v1.0" --restart=Never --env=podIP=$podIP --rm -it --command -- curl -s $podIP:9901/clusters
+++++++
Step3：hostPath存储卷
cat  >volumes-hostpath-demo.yaml <<EOF
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: volumes-hostpath-demo
  labels:
    app: redis
spec:
  containers:
  - name: redis
    image: redis:alpine
    ports:
    - containerPort: 6379
      name: redisport
    securityContext:
      runAsUser: 999
    volumeMounts:
    - mountPath: /data
      name: redisdata
  volumes:
    - name: redisdata
      hostPath:
        path: /appdatas/redis/
        type: DirectoryOrCreate
EOF
kubectl apply -f volumes-hostpath-demo.yaml
kubectl get pods/volumes-hostpath-demo -o jsonpath={.status.hostIP}
kubectl exec -it volumes-hostpath-demo -- redis-cli
127.0.0.1:6379> set mykey "Hi MageEdu"
OK
127.0.0.1:6379> get mykey
"Hi MageEdu"
127.0.0.1:6379> BGSAVE
Background saving started
127.0.0.1:6379> exit

kubectl delete -f volumes-hostpath-demo.yaml && kubectl apply -f volumes-hostpath-demo.yaml 
kubectl get pods/volumes-hostpath-demo -o jsonpath={.status.hostIP}
kubectl exec -it volumes-hostpath-demo -- redis-cli
127.0.0.1:6379> get mykey
"Hi MageEdu"
127.0.0.1:6379> exit

+++++Step4：阿里云的云盘存储++++
cat > pvc-demo.yaml << EOF
# Created-By:"MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    volume.beta.kubernetes.io/storage-provisioner: diskplugin.csi.alibabacloud.com
  finalizers:
  - kubernetes.io/pvc-protection
  name: pvc-demo
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: alicloud-disk-ssd
  volumeMode: Filesystem
EOF

kubectl apply -f pvc-demo.yaml
kubectl get pvc/pvc-demo

cat > redis-with-pvc.yaml  <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: redis-with-pvc
spec:
  containers:
  - name: redis
    image: redis:alpine
    ports:
    - containerPort: 6379
      name: redisport
    volumeMounts:
    - mountPath: /data
      name: redisdata
  volumes:
    - name: redisdata
      persistentVolumeClaim:   # PVC存储卷插件
        claimName: pvc-demo
        readOnly: false
EOF
kubectl apply -f redis-with-pvc.yaml    
kubectl exec -it redis-with-pvc -- redis-cli set site "www.magedu.com"
kubectl exec -it redis-with-pvc -- redis-cli bgsave
kubectl exec -it redis-with-pvc -- ls /data
kubectl delete -f redis-with-pvc.yaml
kubectl apply -f redis-with-pvc.yaml
kubectl exec -it redis-with-pvc -- redis-cli get site
    www.magedu.com    

+++++++++++++configmap++++++++++++
#命令式参数创建ConfigMap
kubectl create configmap demoapp-config --from-literal=demoapp.host='0.0.0.0' --from-literal=demoapp.port='8080'
kubectl get configmaps demoapp-config -o yaml
#命令行加载文件创建ConfigMap
 kubectl create configmap nginx-confs --from-file=./nginx-conf.d/myserver.conf --from-file=status.cfg=./nginx-conf.d/myserver-status.cfg 
 kubectl get configmap nginx-confs -o yaml
 kubectl create configmap nginx-config-files --from-file=./nginx-conf.d/
 kubectl get configmap/nginx-config-files -o yaml
 #: 通过环境变量引用ConfigMap键值
 cat  >configmaps-env-demo.yaml <<EOF
 # Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: ConfigMap
metadata:
  name: demoapp-config
data:
  demoapp.port: "8080"
  demoapp.host: 0.0.0.0
---
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: configmaps-env-demo
spec:
  containers:
  - image: ikubernetes/demoapp:v1.0
    name: demoapp
    env:
    - name: PORT
      valueFrom:
        configMapKeyRef:
          name: demoapp-config
          key: demoapp.port
          optional: false
    - name: HOST
      valueFrom:
        configMapKeyRef:
          name: demoapp-config
          key: demoapp.host
          optional: true
EOF
kubectl apply -f configmaps-env-demo.yaml
kubectl exec configmaps-env-demo -- netstat -tnl 
# 使用configmap存储卷

cat  > configmaps-volume-demo.yaml <<EOF
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: configmaps-volume-demo
spec:
  containers:
  - image: nginx:alpine
    name: nginx-server
    volumeMounts:
    - name: ngxconfs
      mountPath: /etc/nginx/conf.d/
      readOnly: true
  volumes:
  - name: ngxconfs
    configMap:
      name: nginx-config-files
      optional: false
podIP=$(kubectl get pods configmaps-volume-demo -o go-template={{.status.podIP}})
kubectl run pod-$RANDOM --image="ikubernetes/admin-box:v1.1" --restart=Never --env="podIP=${podIP}" --rm -it --command -- /bin/sh
  curl http://${podIP}:8080/nginx-status
kubectl exec configmaps-volume-demo -- ls /etc/nginx/conf.d


+++++ services++++
Step2: ClusterIP类型的Service资源
cat > demoapp.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: demoapp
  labels:
    app: demoapp
spec:
  containers:
  - name: demoapp
    image: nginx
    resources:
    ports:
      - containerPort: 80
EOF
#
cat  > deployment_demoapp.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demoapp
spec:
  selector:
    matchLabels:
      app: demoapp
  replicas: 2
  template:
    metadata:
      labels:
        app: demoapp
    spec:
      containers:
      - name: demoapp
        image: ikubernetes/demoapp:v1.0
        ports:
        - containerPort: 80
EOF
kubectl apply -f deployment_demoapp.yaml 

cat > services-clusterip-demo.yaml <<EOF
# Created-By:"MageEdu <mage@magedu.com>"
kind: Service
apiVersion: v1
metadata:
  name: demoapp
spec:
  selector:
    app: demoapp
  ports:
  - name: http       # 端口名称标识
    protocol: TCP    # 协议，支持TCP、UDP和SCTP
    port: 80          # Service自身的端口号
    targetPort: 80   # 目标端口号，即Endpoint上定义的端口号
EOF

kubectl apply -f services-clusterip-demo.yaml
kubectl get services/demoapp -o wide
kubectl describe services demoapp
kubectl run pod-$RANDOM --image="ikubernetes/admin-box:v1.2" --restart=Never --rm -it  --command -- /bin/bash
  for ((i=0;i<=6;i++)); do curl http://$DEMOAPP_SERVICE_HOST; sleep .1; done

++++++Step3: NodePort类型的Service资源+++
# Created-By:"MageEdu <mage@magedu.com>"
kind: Service
apiVersion: v1
metadata:
  name: demoapp
spec:
  type: NodePort
  selector:
    app: demoapp
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
kubectl apply -f services-nodeport-demo.yaml
kubectl get services/demoapp -o wide
  我们可于集群外部的某一主机上，向集群的任一节点的该NodePort发起请求。请求流量将由services/demoapp-nodeport调度给后端端点。
  该service使用了与前面的services/demoapp相同的标签选择器，因而，无论客户端请求自<SERVICE_IP>:<SERVICE_PORT>进入，
  还是经由<NODE_IP>:<NODE_PORT>进入，都可由该Service一体完成流量调度。

+++++++Step4：LoadBalancer类型的Service+++++
cat >  services-loadbalancer-demo.yaml <<EOF
 # Created-By:"MageEdu <mage@magedu.com>"
kind: Service
apiVersion: v1
metadata:
  name: demoapp
spec:
  type: LoadBalancer
  selector:
    app: demoapp
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
EOF
kubectl apply -f services-loadbalancer-demo.yaml
kubectl get services/demoapp -o wide
  除了原有的流量入口外，services/demoapp此时还能够通过集群分配的外部IP地址（EXTERNAL-IP字段的值）接入外部流量，
  并将其转发至各节点的NodePort之上。接下来，我们可于集群外部访问该LB IP上暴露的服务，对于services/demoapp来说，
  在本地主机上打开浏览器，键入上面的外部地址即可。
++++++++++++++++++++Step5：基于ClusterDNS的服务发现+++++++++++++++
kubectl run pod-$RANDOM --image="ikubernetes/admin-box:v1.2" --restart=Never --rm -it  --command -- /bin/bash
  cat /etc/resolv.conf 
  nslookup -query=A demoapp  #解析demoapp的A记录
  host -t SRV demoapp      #查询服务地址
  host -t PTR ${DEMOAPP_SERVICE_HOST}   #查询反向解析地址  ip查域名
  host -t NS cluster.local.              #查询dns服务名称
  host -t A kubernetes.default.svc       #k8s服务的地址

  ++++++++++++++配置Pod的DNS策略 +++++++++++++
    Default：从运行在的节点继承DNS名称解析相关的配置；
    ClusterFirst：于集群DNS服务上解析集群域内的名称，其他域名的解析则交由从节点继承而来的上游名称服务器；
    ClusterFirstWithHostNet：专用于在设置了hostNetwork的Pod对象上使用的ClusterFirst策略，任何配置了hostNetwork的Pod对象都应该显式使用该策略；
    None：用于忽略Kubernetes集群的默认设定，而仅使用由dnsConfig自定义的配置；
Pod资源的自定义DNS配置要通过嵌套于spec.dnsConfig字段中的如下几个字段进行，它们的最终生效结果要结合dnsPolicy的定义生成。
    nameservers <[]string>：DNS名称服务器列表，它附加于由dnsPolicy生成的DNS名称服务器之后；
    searches <[]string>：DNS名称解析时的搜索域，它附加由于dnsPolicy生成的搜索域之后；
    options <[]Object>：DNS解析选项列表，它将会同dnsPolicy生成的解析选项合并成最终生效的定义；


cat  > pod-with-customed-dnspolicy.yaml <<EOF
# Created-By:"MageEdu <mage@magedu.com>"
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-customed-dnspolicy
spec:
  containers:
  - name: demo
    image: ikubernetes/demoapp:v1.0
    imagePullPolicy: IfNotPresent
  dnsPolicy: None
  dnsConfig:
    nameservers:
    - 172.21.0.10
    searches: 
    - svc.cluster.local
    - cluster.local
    - magedu.com
    options:
    - name: ndots
      value: "5"
EOF
kubectl apply -f pod-with-customed-dnspolicy.yaml
kubectl exec -it pod-with-customed-dnspolicy -- /bin/sh
    cat /etc/resolv.conf 
    nslookup -query=A demoapp

+++++++++++++Step7：External Service+++++++++++++++
Kubernetes将那些ClusterIP字段值为“None”的Service资源称为Headless Service，该类Service的请求流量无须kube-proxy处理，也不会有负载均衡和路由相关的iptables或ipvs规则
我们把无标签选择器的第一种情形（使用CNAME记录）的Headless Service当作一种独立的Service类型使用，即ExternalName Service，
而将那些把Service名称使用A记录解析为端点IP地的情况则统一称为Headless Service。

cat > externalname-mageedu-svc.yaml <<EOF
kind: Service
apiVersion: v1
metadata:
  name: externalname-mageedu
spec:
  type: ExternalName
  externalName: www.magedu.com
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  selector: {}
EOF
kubectl apply -f externalname-mageedu-svc.yaml
kubectl run pod-$RANDOM --image="ikubernetes/admin-box:v1.2" --restart=Never --rm -it  --command -- /bin/bash
  nslookup -query=A externalname-mageedu 
++++++++++++++++++Step8: Headless Service+++++++++++
cat > demoapp-headless-svc.yaml <<EOF
kind: Service
apiVersion: v1
metadata:
  name: demoapp-headless-svc
spec:
  clusterIP: None
  selector:
    app: demoapp
  ports:
  - port: 80
    targetPort: 80
    name: http
EOF
kubectl apply -f demoapp-headless-svc.yaml
    nslookup -query=A demoapp-headless-svc 
    for ip in $(host -t A demoapp-headless-svc | awk '{print $4}'); do host -t PTR $ip; done

+++++++++++++++++++++++++++++++++++Job和Cronjob控制器 +++++++++++++++++++++++++++++++++
++++Step2: Job控制器 +++

定义Job资源时，spec字段内嵌的必要字段仅有template一个，Job会为其Pod对象自动添加job-name=JOB_NAME和controller-uid=UID标签，
并使用标签选择器完成对controller-uid标签的关联。
cat > job-demo.yaml <<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: batch/v1
kind: Job
metadata:
  name: job-demo
spec:
  template:
    spec:
      containers:
      - name: myjob
        image: ikubernetes/admin-toolbox:v1.0
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c", "sleep 60"]
      restartPolicy: Never
  completions: 3
  ttlSecondsAfterFinished: 3600   # Job的留存时长，超过该时长将被自动删除；  
  backoffLimit: 3   # 将作业标记为失败状态之前的重试次数，默认值为6；
  activeDeadlineSeconds: 300   # Job的最大活动时长，超过该时长仍未成功结束时将被标记为失败；
EOF

kubectl apply -f job-demo.yaml
kubectl get jobs/job-demo -o wide
kubectl get jobs/job-demo -o wide
kubectl get pods -l job-name=job-demo
等作业任务完成后，jobs/job-demo所创建的所有Pod对象将处于“Completed”状态。
    NAME             READY   STATUS      RESTARTS   AGE
    job-demo-42gjf   0/1     Completed   0          13m
    job-demo-g9hdt   0/1     Completed   0          11m
    job-demo-pcpvd   0/1     Completed   0          12m

+++++Step3：并行式Job++++
cat > job-para-demo.yaml <<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: batch/v1
kind: Job
metadata:
  name: job-para-demo
spec:
  template:
    spec:
      containers:
      - name: myjob
        image: ikubernetes/admin-toolbox:v1.0
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c", "sleep 60"]
      restartPolicy: OnFailure
  completions: 10
  parallelism: 2
  ttlSecondsAfterFinished: 3600
  backoffLimit: 3
  activeDeadlineSeconds: 1200
EOF
kubectl apply -f job-para-demo.yaml
kubectl get jobs/job-para-demo -o wide
kubectl get pods -l job-name=job-para-demo
kubectl patch jobs/job-para-demo -p '{"spec":{"parallelism":5}}'
kubectl get pods -l job-name=job-para-demo

++++Step5：Cronjob控制器++++
cat >  cronjob-demo.yaml <<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com  
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cronjob-demo
spec:
  schedule: "*/2 * * * *"
  jobTemplate:
    metadata:
      labels:
        app: mycronjob-jobs
    spec:
      parallelism: 1
      completions: 1
      ttlSecondsAfterFinished: 3600
      backoffLimit: 3
      activeDeadlineSeconds: 60
      template:
        metadata:
          labels:
            app: cronjob-demo-pod
        spec:
          containers:
          - name: myjob
            image: "ikubernetes/admin-toolbox:v1.0"
            command:
            - /bin/sh
            - -c
            - date; echo Hello from CronJob, sleep a while…; sleep 10;
          restartPolicy: OnFailure
  startingDeadlineSeconds: 300
EOF
kubectl apply -f cronjob-demo.yaml
+++++++++++++++++++++++理解和使用ServiceAccount+++++++++++++++++++++++++++++++++
+++++Step2：ServiceAccount自动化+++
为每个名称空间自动创建一名为default的ServiceAccount，并为每个未指定ServiceAccount的Pod自动附加其所属名称空间上的serviceaccounts/default对象。
ServiceAccount将其适用的名称空间、认证到API Server的Token，以及信任的CA证书等敏感信息存储于专用的Secret对象中。
Pod会以存储卷的形式附加其使用的ServiceAccount相关的Secret对象，并由各容器挂载至默认的/var/run/secrets/kubernetes.io/serviceaccount目录下。

kubectl run pod-01 --image="ikubernetes/demoapp:v1.0" --restart=Never --rm -it --command -- /bin/sh
kubectl get pods/pod-01 -o yaml
    apiVersion: v1
    kind: Pod
    metadata:
      ……
      name: pod-01
      namespace: handson-22084b6dfd81c0d5e414222b5f461693
      ……
    spec:
      containers:
      - command:
        - /bin/sh
        image: ikubernetes/demoapp:v1.0
        imagePullPolicy: IfNotPresent
        name: pod-01
        ……
        volumeMounts:
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount    # Secret存储卷的挂载点
          name: default-token-6dhx2
          readOnly: true
      ……
      serviceAccount: default     # 使用的ServiceAccount对象，已废弃该字段，由下面的字段所替换；
      serviceAccountName: default    # 使用的ServiceAccount对象；
      ……
      volumes:
      - name: default-token-6dhx2    # ServiceAccount对象相关的Secret存储卷
        secret:
          defaultMode: 420
          secretName: default-token-6dhx2    # Secret对象的名称
    ……

kubectl describe serviceaccounts/default
ls -1 /var/run/secrets/kubernetes.io/serviceaccount
    ca.crt      # CA的证书文件
    namespace   # 保存有适用的名称空间的文件
    token       # 保存有认证到API Server的Token的文件 

+++Step3：创建ServiceAccount对象+++
该Secret对象属于特殊的kubernetes.io/service-account-token类型，它包含ca.crt、namespace和token3个数据项
kubectl create serviceaccount mysa01
kubectl get serviceaccounts/mysa01 -o jsonpath={.secrets[0].name}
      mysa01-token-2qfbl     # 名称后缀随机生成；
kubectl get secrets $(kubectl get serviceaccounts/mysa01 -o jsonpath={.secrets[0].name}) -o yaml

++++Step4：ServiceAccount资源规范+++
cat > serviceaccount-demo.yaml <<EOF
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
automountServiceAccountToken: true
EOF
kubectl apply -f serviceaccount-demo.yaml
kubectl get serviceaccounts/admin -o jsonpath={.secrets[0].name}
kubectl get serviceaccounts/default -o yaml
ServiceAccount资源还可以基于spec.imagePullSecret字段附带一个由下载镜像专用的Secret资源组成的列表，
用于让Pod对象在创建容器时从私有镜像仓库下载镜像文件之前完成身份认证。
kubectl get serviceaccounts/default -o yaml
    ……
    imagePullSecrets:
    - name: acr-credential-d8cb8c4195a48ca6dbefe56f6399ef1f
    - name: acr-credential-27b7396c3bc51e6df6ffbf8189648a39
    - name: acr-credential-2b442760ae1031e228cdeea8831392a1
    - name: acr-credential-278a682b6f1cc97ae2bdfefacfa9b8a8
    - name: acr-credential-e0de33735568ed2f86e4b7795df1e710
++++Step5：配置Pod使用自定义的ServiceAccount++++
cat > pod-with-sa.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-sa
spec:
  containers:
  - name: adminbox
    image: ikubernetes/admin-toolbox:v1.0
    imagePullPolicy: IfNotPresent
  serviceAccountName: admin
EOF
kubectl apply -f pod-with-sa.yaml
kubectl exec -it pod-with-sa -- /bin/sh
cd /var/run/secrets/kubernetes.io/serviceaccount;curl --cacert ./ca.crt  \ 
   -H "Authorization: Bearer $(cat ./token)" https://kubernetes.default.svc.cluster.local/api/v1/namespaces/$(cat ./namespace)/pods/

      {
        "kind": "Status",
        "apiVersion": "v1",
        "metadata": {

        },
        "status": "Failure",
        "message": "pods is forbidden: User \"system:serviceaccount:handson-22084b6dfd81c0d5e414222b5f461693:admin\" cannot list resource \"pods\" in API group \"\" in the namespace \"handson-22084b6dfd81c0d5e414222b5f461693\"",
        "reason": "Forbidden",
        "details": {
          "kind": "pods"
        },
        "code": 403
      }
kubectl create rolebinding admin-binding-admin --clusterrole=view --serviceaccount=handson-22084b6dfd81c0d5e414222b5f461693:admin
kubectl create rolebinding admin-binding-admin --clusterrole=view --serviceaccount=default:admin #授权
若能够正常显示，则表示授权成功。这也表明了使用特定ServiceAccount的Pod可以使用该ServiceAccount上获得的相应权限。






============================使用 Kubectl 部署 web 服务到 K8s 集群===============
registry.cn-shanghai.aliyuncs.com/workbench1459088147016887/handsonack_test:3
+++++++++++++++++++++++++++++++++++
deployment.yaml:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  namespace: handson-42808f57ab97825591fac00ba3e829ca
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      name: myapp
  template:
    metadata:
      labels:
        name: myapp
      namespace: handson-42808f57ab97825591fac00ba3e829ca
    spec:
      containers:
        - name: myapp
          image: registry.cn-shanghai.aliyuncs.com/workbench_1459088147016887/handson_ack_test:3
          ports:
            - containerPort: 8080

=========================service=============
service.yaml:
 
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  namespace: handson-42808f57ab97825591fac00ba3e829ca
spec:
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: NodePort
  selector:
    name: myapp

=========================ingress==========================
example-ingress.yaml:

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: example-ingress
  namespace: handson-42808f57ab97825591fac00ba3e829ca
spec:
  rules:
  - http:
      paths:
      - path: /welcome
        backend:
          serviceName: myapp-service
          servicePort: 8080

***********
kubectl delete ingress example-ingress
kubectl delete service myapp-service
========================================================================


aliyun:  nignx_ingress-lb
    apiVersion: v1
    kind: Service
    metadata:
    annotations:
    labels:
        app: nginx-ingress-lb
    name: nginx-ingress-lb
    namespace: kube-system
    spec:
    externalTrafficPolicy: Local
    ports:
    - name: http
        port: 80
        protocol: TCP
        targetPort: 80
    - name: https
        port: 443
        protocol: TCP
        targetPort: 443
    selector:
        app: ingress-nginx
    type: LoadBalancer


# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-conf
data:
  default.conf: |
    server {
      listen       80;
      server_name  localhost;
      location / {
          root   /usr/share/nginx/html;
          index  index.html index.htm;
      }
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
          root   /usr/share/nginx/html;
      }
      location /nginx_status {
        stub_status;
        allow 127.0.0.1; 
        deny all;   
      }
    }
---

=========================================
【体验】多容器 Pod 及其设计模式
https://start.aliyun.com/handson/0iNJ9RDH/Understand_Multi-Container_Pod_design_patterns
Kubernetes中运行多种不同类型的多容器Pod，这包括：
Sidecar
Ambanssdor
Adapter
Initializer
===========================
envoy.yaml

admin:
  access_log_path: /tmp/admin_access.log
  address:
    socket_address: { address: 0.0.0.0, port_value: 9901 }

static_resources:
  listeners:
  - name: listener_0
    address:
      socket_address: { address: 0.0.0.0, port_value: 80 }
    filter_chains:
    - filters:
      - name: envoy.http_connection_manager
        config:
          stat_prefix: ingress_http
          codec_type: AUTO
          route_config:
            name: local_route
            virtual_hosts:
            - name: local_service
              domains: ["*"]
              routes:
              - match: { prefix: "/" }
                route: { cluster: mageedu_service }
          http_filters:
          - name: envoy.router
  clusters:
  - name: mageedu_service
    connect_timeout: 0.25s
    type: STATIC
    lb_policy: ROUND_ROBIN
    load_assignment:
      cluster_name: mageedu_service
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: 127.0.0.1
                port_value: 8080

===================================
sidecar-container-demo.yaml
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-container-demo
spec:
  containers:
  - name: proxy   # 容器一
    image: envoyproxy/envoy-alpine:v1.16.2
    command: ['/bin/sh','-c']
    args: ['sleep 3 && envoy -c /etc/envoy/envoy.yaml']
    lifecycle:
      postStart:
        exec:
          command: ['/bin/sh','-c','wget -O /etc/envoy/envoy.yaml https://code.aliyun.com/MageEdu/kubernetes-ckad/raw/master/tutorials/handson/Understand_Multi-Container_Pod_design_patterns/envoy.yaml']
  - name: demoapp    # 容器二
    image: ikubernetes/demoapp:v1.0
    imagePullPolicy: IfNotPresent
    env:
    - name: HOST
      value: "127.0.0.1"
    - name: PORT
      value: "8080"

*******
kubectl apply -f sidecar-container-demo.yaml
kubectl get pods -o wide
kubectl exec sidecar-container-demo -c proxy -- netstat -tnlp
envoyIP=$(kubectl get pods/sidecar-container-demo -o jsonpath={.status.podIP})
kubectl run client-$RANDOM --image="ikubernetes/admin-toolbox:v1.0" --restart=Never --rm -it --env=envoyIP=${envoyIP} --command -- /bin/sh
    curl -I http://$envoyIP/


===============
ambassador-container-demo.yaml
# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kind: Pod
metadata:
  name: ambassador-container-demo
spec:
  containers:
  - name: curl
    image: ikubernetes/admin-toolbox:v1.0
    command: ["sleep", "999999"]
  - name: ambassador
    image: bitnami/kubectl:1.21
    command: ["/bin/sh","-c","kubectl proxy"]
    args:    # 传递给 kubectl proxy 的选项，若需要改变默认监听的tcp/8001端口，可以额外附加“--port=NUM”选项；
    - --server="https://kubernetes.default.svc"
    - --certificate-authority="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
    - --token="$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"
    - --accept-paths='^.\*'

*****
kubectl apply -f ambassador-container-demo.yaml
kubectl exec -it ambassador-container-demo -c curl -- /bin/sh
    curl localhost:8001/
    exit

===========================================
adapter-container-demo.yaml

apiVersion: v1
kind: Pod
metadata:
  name: adapter-container-demo
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /etc/nginx/conf.d/
      name: nginx-conf
      readOnly: true
  - name: adapter
    image: nginx/nginx-prometheus-exporter:0.9.0
    args: ["-nginx.scrape-uri","http://localhost/nginx_status"]
    ports:      # nginx-prometheus-exporter默认监听tcp/9113端口
    - name: exporter
      containerPort: 9113   
  volumes:
  - name: nginx-conf
    configMap:
      name: nginx-conf
      items:
      - key: default.conf
        path: default.conf

*************
kubectl apply -f adapter-container-demo.yaml
kubectl get pods/adapter-container-demo -o wide
adapterIP=$(kubectl get pods/adapter-container-demo -o jsonpath={.status.podIP})
kubectl run client-$RANDOM --image="ikubernetes/admin-toolbox:v1.0" --restart=Never --rm -it --env=adapterIP=${adapterIP} --command -- /bin/sh
    curl -I http://$adapterIP:9113/metrics/
    exit



=======================================
init_containter_demo.yaml

# Create By: "MageEdu <mage@magedu.com>"
# Site: www.magedu.com
apiVersion: v1
kbles-init
    image: ikubernetes/admin-toolbox:v1.0
    imagePullPolicy: IfNotPresent
    command: ['/bin/sh','-c']
    args: ['iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80']
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
  containers:
  - name: demo
    image: ikubernetes/demoapp:v1.0
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80

*****
kubectl apply -f init-container-demo.yaml
kubectl describe pods/init-container-demo
kubectl exec init-container-demo -- iptables -t nat -vnL
podIP=$(kubectl get pods/init-container-demo -o jsonpath={.status.podIP})
kubectl run client-$RANDOM --image="ikubernetes/admin-toolbox:v1.0" --restart=Never --rm -it --env=podIP=${podIP} --command -- /bin/sh
    curl http://$podIP:8080/
    exit


kubectl get pods,configmaps
kubectl delete pods,configmaps --all --force --grace-period=0




ctr i -h查看帮助

ctr i pull docker.io/library/redis:alpine 拉取镜像（默认不带镜像仓库地址） docker pull docker.io/library/redis:alpine docker仓库会自动补全

ctr i ls 查看镜像

ctr ns 查看命名空间 c 创建 ls 查看  rm删除 label设置标签

docker tag redis:alpine registry.cn-hangzhou.aliyuncs.com/imooc/redis:alpine将镜像重新打标签

docker push registry.cn-hangzhou.aliyuncs.com/imooc/redis:alpin上传镜像

ctr i pull registry.cn-hangzhou.aliyuncs.com/imooc/redis:alpin 拉取镜像

ctr i ls查看镜像

ctr run -t -d registry.cn-hangzhou.aliyuncs.com/imooc/redis:alpin redis 启动容器

ctr c ls查看当前运行容器

ctr t ls 查看当前运行任务

ctr t kill 杀掉当前任务

ctr t rm 删除任务

ctr c rm 删除容器

ctr -n default t ls 查看当前命名空间任务

k8s给containerd提供crictl命令

crictl images 查看镜像

crictl ps 查看进程

crictl pod 查看pod

crictl logs 查看日志

crictl exec 进入容器

alias docker=crictl 将crictl转变为docker命令操作ctr



安装 kubernetes-dashboard
最新版本查看地址：https://github.com/kubernetes/dashboard/releases

1.下载
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml

2.修改配置文件
默认 Dashboard 只能集群内部访问，修改 Dashboard 配置文件的 Service 为 NodePort 类型，暴露到外部访问：

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30001
  selector:
    k8s-app: kubernetes-dashboard

---
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
3.执行配置文件
kubectl apply -f recommended.yaml
1
查看 kubernetes-dashboard：

kubectl get pods -n kubernetes-dashboard
1
NAME                                        READY   STATUS    RESTARTS   AGE
dashboard-metrics-scraper-8c47d4b5d-4blld   1/1     Running   0          21s
kubernetes-dashboard-5676d8b865-2t9mz       1/1     Running   0          21s
1
2
3
4.创建 dashboard 的 serviceaccount
kubectl create serviceaccount dashboard-admin -n kube-system
1
查看 serviceaccount：

kubectl get serviceaccount -n kube-system | grep dashboard-admin
1
5.创建 clusterrolebinding
创建 clusterrolebinding 绑定 clusterrole 使用 serviceaccount 认证：

kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
1
6.生成 token
kubectl create token dashboard-admin -n kube-system --duration=87600h
eyJhbGciOiJSUzI1NiIsImtpZCI6InNka1d3VVNYaDF3N2dfQ1B6Q3Ruc2cxWjZRbUxhU1lYN0dDbVZ4WmR5VlUifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxOTcwNTY0OTQ4LCJpYXQiOjE2NTUyMDQ5NDgsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiI4MDgxOGEyNy0yNTkyLTRmMDctODdlMi1mZmZmYTRjMTE2ZjAifX0sIm5iZiI6MTY1NTIwNDk0OCwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.PdWWzm1vZKRKPnBqds5LSeFvtMdjl4R85zml03LVebP1Eh52oZKy35gLHOvosHTSTXaWELIQcuMYZed7DZjudvxQOKmmKptKRL17EiOmML528OYD_SlraQjVjwFh5LI523mKSioHLp_NhfJuswqJFUoAoB_J49cOGKHjU3TcTvwzUJz1ywQxSx7L5LZ42XJn2q-fXYzhf3fOe6VLhXeibPey_SFFc20d7YvfDfLMTYnPe69KX9U9QCD79qVaGPOnsHGV2GWu3L7BlL-FZWz8uzFT37qEo35V1fnqmuEjtNs-tQVgcU0qtnprTV-bEckWbfdVyPTDoCwdmEP0XbXyTQ
1
2
7.查看生成的 token
kubectl get secret -n kube-system | grep dashboard-admin
1
8.查看令牌
kubectl describe secret dashboard-admin-token-23uyr  -n kube-system
1
9.访问
输入访问地址：https://192.168.1.28:30001，输入 token，点击【登录】：
————————————————
版权声明：本文为CSDN博主「奔跑吧邓邓子」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u012069313/article/details/125201017


openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed ‘s/^.* //’ #获取hash
————————————————
版权声明：本文为CSDN博主「茁壮成长的麦芽」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_34953582/article/details/125400579


config.toml


Registry Configuration
Here is a simple example for a default registry hosts configuration. Set config_path = "/etc/containerd/certs.d" in your config.toml for containerd. Make a directory tree at the config path that includes docker.io as a directory representing the host namespace to be configured. Then add a hosts.toml file in the docker.io to configure the host namespace. It should look like this:

$ tree /etc/containerd/certs.d
/etc/containerd/certs.d
└── docker.io
    └── hosts.toml

$ cat /etc/containerd/certs.d/docker.io/hosts.toml
server = "https://docker.io"

[host."https://registry-1.docker.io"]
  capabilities = ["pull", "resolve"]
To specify a custom certificate:

$ cat /etc/containerd/certs.d/192.168.12.34:5000/hosts.toml
server = "https://192.168.12.34:5000"

[host."https://192.168.12.34:5000"]
  ca = "/path/to/ca.crt"
See docs/hosts.md for the further information.




curl -Lo minikube  https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/latest/minikube-linux-amd64


========================== istio =================================
1.   安装：
curl -L https://istio.io/downloadIstio | sh -
curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.14.3 TARGET_ARCH=x86_64 sh -

mv   istio-1.14.3 /usr/local
cat >>/etc/profile <<EOF   #加入path
export PATH=$PATH:/usr/local/istio-1.14.3/bin
EOF

加载 ip_tables 支持  istio_ingressgateway
lsmod | grep ip_tables 来完成。若要显式加载该模块，可执行 sudo modprobe ip_tables
cat <<EOF | sudo tee /etc/modules-load.d/istio.conf
ip_tables
iptable_filter
EOF

modprobe ip_tables
modprobe iptable_filter

cp  istioctl /usr/bin   #加入运行环境， 可用istioctl
istioctl  profile  list   #查看内置 profile 表

istioctl install --set profile=demo -y 
istioctl ps #  proxy-status    查看proxy的状态  是否下发完毕
istioctl pc all   # proxy-config   查看proxy的config配置

kubectl label namespace default istio-injection=enabled
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml


kubectl  apply  -f  /usr/local/istio-1.14.3/samples/addons

kubectl edit  svc   istio-ingressgateway -n istio-system
在clusterIPs 后新增 外部地址 ， 看通过外部地址访问
    externalIPs:
    - 192.168.110.92




========================================
-----------------------------------------------------------------------
 2024年1月30日   ubuntu   安装  docker
 # step 1: 安装必要的一些系统工具
sudo apt-get update
sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common
# step 2: 安装GPG证书
curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
# Step 3: 写入软件源信息
sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
# Step 4: 更新并安装Docker-CE
sudo apt-get -y update
sudo apt-get -y install docker-ce

# 安装指定版本的Docker-CE:
# Step 1: 查找Docker-CE的版本:
# apt-cache madison docker-ce
#   docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages
#   docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages
# Step 2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.1~ce-0~ubuntu-xenial)
# sudo apt-get -y install docker-ce=[VERSION]

# huawei yun
1、若您安装过docker，需要先删掉，之后再安装依赖:
sudo apt-get remove docker docker-engine docker.io
sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
2、根据版本不同，运行公钥，添加软件仓库。您使用的发行版： 
Debian
信任Docker的GPG公钥:
curl -fsSL https://mirrors.huaweicloud.com/docker-ce/linux/debian/gpg | sudo apt-key add -
对于amd64架构的计算机，添加软件仓库:
sudo  
3、更新索引文件并安装
sudo apt-get update
sudo apt-get install docker-ce

modprobe  br_netfilter

cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF
sysctl --system


#echo 1 > /proc/sys/net/ipv4/ip_forward





apt install docker-ce  docker-ce-cli  containerd.io  -y


cat  /etc/containerd/config.toml
   add：
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
          endpoint = ["https://docker.mirrors.ustc.edu.cn", "https://registry.docker-cn.com"]

        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."registry.k8s.io"]
          endpoint = ["https://registry.aliyuncs.com/google_containers"]

        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."registry.magedu.com"]
          endpoint = ["https://registry.magedu.com"]

chmod  0640  /etc/containerd/config.toml

cat  /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false

chmod 0644 /etc/crictl.yaml

systemctl  start  docker
systemctl  start  containerd

#disable swap
swapoff -a 
# edit  /etc/fstab  delete  swap  set

kubernetes:
apt-get update && apt-get install -y apt-transport-https
curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/Release.key |
    gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/ /" |
    tee /etc/apt/sources.list.d/kubernetes.list
apt-get update
apt-get install -y kubelet kubeadm kubectl
 
kubeadm config print init-defaults --kubeconfig clusterconfiguration  >> kubeadm.yaml
vi kubeadm.yaml
     imageRepository: registry.aliyuncs.com/google_containers
kubeadm config images list --config kubeadm.yaml
kubeadm config images pull --config kubeadm.yaml

 vi /etc/netplan/01-network-manager-all.yaml 
 systemctl disable ufw
 apt install lrzsz chronyd
 systemctl start chronyd
 systemctl enable  chronyd
 chronyc tracking 
 vi /etc/fstab
 apt install
 apt-get remove docker docker-engine docker.io
 sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common -y
 chronyc tracking 
 
 apt-get update && apt-get install -y apt-transport-https lrzsz wget curl gnupg-agent software-properties-common apt-transport-hppts ca-certificates
 apt install apt-transport-https
 wget https://mirrors.huaweicloud.com/docker-ce/linux/ubuntu/gpg
 apt-key add gpg
 
 apt-add-repository "deb [arch=amd64] https://mirrors.huaweicloud.com/docker-ce/linux/ubuntu jammy stable"
 apt install containerd.io
 
 cp crictl.yaml  /etc/
 cp containerd-config.toml  /etc/containerd/config.toml 
 systemctl restart containerd
 systemctl enable containerd
 systemctl status containerd
 
 
 apt-get update && apt-get install -y apt-transport-https
 curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.29/deb/Release.key |     gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
 echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.29/deb/ /" |     tee /etc/apt/sources.list.d/kubernetes.list
 apt update
 
 apt install kubelet kubeadm kubectl
 systemctl status kubelet
 
 
 kubeadm config print init-defaults > kubeadm.yaml
 
  vi kubeadm.yaml
 
 kubeadm config images list --config kubeadm.yaml
 kubeadm config images pull  --config kubeadm.yaml
 crictl pull docker.io/calico/node:v3.26.4
