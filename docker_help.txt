docker  相关命令
容器时间同步加参数
-v /etc/localtime:/etc/localtime:ro

	1. 关闭selinux
		sudo setenforce 0
		sudo sed -i 's/enforcing/permissive/g' /etc/selinux/config 
		停防火墙
		systemctl stop firewalld
		systemctl disable firewalld
		systemctl disable firewalld
		iptables -F   # 清除 
	 	iptables -L   # 显示
	2.	docker安装
		
		cat >> /etc/yum.repos.d/docker.repo <<EOF
		[docker-repo]
		name=Docker Repository
		baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7
		enabled=1
		gpgcheck=0
		EOF
			Kubernetes 1.6还没有针对docker 1.13和最新的docker 17.03上做测试和验证，所以这里安装Kubernetes官方推荐的Docker 1.12版本。
			[root@server2 ~]# yum list docker-engine --showduplicates ##查看docker版本
			[root@server2 ~]# yum install -y docker-engine-1.12.6-1.el7.centos.x86_64  ##安装docker
			直接这样装会报错。需要同时安装docker-engine-selinux-1.12.6-1.el7.centos.noarch.rpm，如下
			[root@server2 ~]# yum install -y docker-engine-1.12.6-1.el7.centos.x86_64 docker-engine-selinux-1.12.6-1.el7.centos.noarch
			##或者将这两个包下载下来，再一起安装
	3.	国内加速
		cat > /etc/docker/daemon.json <<EOF
		{
		"registry-mirrors": ["https://9npjh5s8.mirror.aliyuncs.com"]
		}
		EOF
		systemctl daemon-reload
        systemctl enable docker
        systemctl start docker    
    4. 开启转发 
		 iptables -P FORWARD ACCEPT
			 可在docker的systemd unit文件中以ExecStartPost加入上面的命令：
			  ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT
		或：
		cat << EOF > /etc/sysctl.d/k8s.conf
		net.ipv4.ip_forward = 1
		net.bridge.bridge-nf-call-ip6tables = 1
		net.bridge.bridge-nf-call-iptables = 1
		vm.swappiness=0
		EOF

		modprobe br_netfilter
		echo "modprobe br_netfilter" >> /etc/rc.local

		sysctl -p /etc/sysctl.d/k8s.conf 

		cat /proc/sys/net/bridge/bridge-nf-call-iptables 
		cat /proc/sys/net/bridge/bridge-nf-call-ip6tables 
		cat /proc/sys/net/ipv4/ip_forward   
    5. docker 命令
		docker  images  -a  all  /  --no-trunc  / -q  only ID  -看镜像
		docker  search --no-trunc 查找仓库
		docker login 
		docker logout
		docker pull   
		docker push 
		docker ps  -a all  / -l laster
		docker top
		docker logs  -f follow  --tail 
		docker events 
		docker history
		docker build -t='container name :tag" .    --no-cache 通过dockerfile 生产镜像
		docker attach  附加容器  ctrl+p  ctrl+q
		docker  inspect  查看容器
		docker port 容器 查看端口
		docker  rm  -f  force / -v volumes /-l link 删除容器
		docker  rmi -f  force  删除镜像
		docker start
		docker pause		
		docker stop
		docker kill
		docker  restart
		docker rename
		docker  run  -d / -t  -i  -P -p 
		docker  exec -d -t -i
		docker  tag 
		docker  info
		docker version  
		docker  commit 容器TO 镜像
		docker load IMAGE
		docker save IMAGE
		docker export CONTAINER
		docker import  CONTAINER
		docker network
		docker swarm
		docker node
		
        dockerfile  entrypoint [" ", ”  ]   CMD [""]  -命令参数替换
    
	6.	网桥管理命令
		yum  install -y  bridge-utils  安装网桥工具包 
		
		brctl show  查看桥架设备
		brctl addbr 新建桥
		brctl delbr	删除桥
		brctl addif 加interface to 桥
		brctl delif  删除interface from 桥
		
		建网桥
			brctl addbr bridge0 
			ip addr add 192.168.5.1/24 dev bridge0 
			ip link set dev bridge0 up 
		route -n  -查看路由表
		
		关闭网桥：
			brctl delif br0 eth0;
			ifconfig bro  down;
			brctl delbr br0;

	7.	docker  服务启动参数
			/etc/sysconfig/docker -启动配置文件
					-H tcp://0.0.0.0:2375    提供远程访问
					可本机定义变量  DOCKER_HOST  可本机访问
				或：-H unix:///var/run/docker.sock	 默认本机
		centos7: vi  /usr/lib/systemd/system/docker.services 加如下内容：- 指定远程访问及本机访问
					 -H tcp://0.0.0.0:2375 \
					 -H unix:///var/run/docker.sock \			
			
	8.	docker 客户端远程访问 ， 可以定义环境变量
			export DOCER_HOST=”tcp://192.168.x.x:2375“	
			noset   --删除环境变量定义
			env   显示环境变量
		 
		/etc/sysconfig/docker-storage
		/etc/sysconfig/docker-network
		/etc/docker/daemon.json 
	
	9.  docker info  查看docker　id  如相同 需删除 /etc/docker/key.json  可重新生产docker id
		    rm -f  /etc/docker/key.json
	
	10.	pipework:
				brctl  addbr  br0   
				ip link set dev  br0 up
				ip address add  192.168.1.10/24 dev br0
				ip address del 192.168.1.10/24 dev eth0 
				brctl addif  br0  eth0  将宿主机网卡绑定br0 上
			
				ip route del default
				ip route add default   via  192.168.1.24 dev br0            为br0 设置路由
			
			git clone  https://github.com/jpetazzo/pipework
			cp ~/pipework/pipework /usr/local/bin   将 PIPEWORK  复制的PATH 路径中
			启动容器：
				docker  run  -itd  --net=none  --name=test  centos:laster  /bin/bash
				pipework br0  test  192.168.1.88/24@192.168.1.254   设置容器IP 及网关  该地址只在运行状态有效， 容器停止后IP 将丢失
	
			进入容器查看IP
				docker attach  test
				docker exec   test   ip add  show  
				
				pipework
				ip address show


jenkins
    docker pull jenkins    
    chmod 777 -R  /root/jenkins
    docker run -p 8080:8080 -p 50000:50000 -v /root/jenkins:/var/jenkins_home jenkins

gitLab   
    
      docker pull gitlab/gitlab-ce  
      wget https://raw.githubusercontent.com/sameersbn/docker-gitlab/master/docker-compose.yml
      docker-compose up

      Start GitLab using:

      docker-compose up
      Alternatively, you can manually launch the gitlab container and the supporting postgresql and redis containers by following this three step guide.

      Step 1. Launch a postgresql container

      docker run --name gitlab-postgresql -d \
      --env 'DB_NAME=gitlabhq_production' \
      --env 'DB_USER=gitlab' --env 'DB_PASS=password' \
      --env 'DB_EXTENSION=pg_trgm' \
      --volume /srv/docker/gitlab/postgresql:/var/lib/postgresql \
      sameersbn/postgresql:9.6-2
      Step 2. Launch a redis container

      docker run --name gitlab-redis -d \
      --volume /srv/docker/gitlab/redis:/var/lib/redis \
      sameersbn/redis:latest
      Step 3. Launch the gitlab container

      docker run --name gitlab -d \
      --link gitlab-postgresql:postgresql --link gitlab-redis:redisio \
      --publish 10022:22 --publish 10080:80 \
      --env 'GITLAB_PORT=10080' --env 'GITLAB_SSH_PORT=10022' \
      --env 'GITLAB_SECRETS_DB_KEY_BASE=long-and-random-alpha-numeric-string' \
      --env 'GITLAB_SECRETS_SECRET_KEY_BASE=long-and-random-alpha-numeric-string' \
      --env 'GITLAB_SECRETS_OTP_KEY_BASE=long-and-random-alpha-numeric-string' \
      --volume /srv/docker/gitlab/gitlab:/home/git/data \
      sameersbn/gitlab:10.5.6
      Please refer to Available Configuration Parameters to understand GITLAB_PORT and other configuration options

      NOTE: Please allow a couple of minutes for the GitLab application to start.

      Point your browser to http://localhost:10080 and set a password for the root user account



docker
  
    docker swarm init --advertise-addr  192.168.1.12
    docker swarm join     --token SWMTKN-1-24uw3xrfo0for1gb6tvqn0vomlnp9vgr05kpe03p9iknfll4qo-cv1jgkz320ydu09flw990urec   192.168.1.12:237
    docker network create -d overlay  --subnet  10.25.0.0/24  overnet
    docker service create  --network overnet  ngin		
	

		
docker 监控：
		容器准备：
		docker  pull tutum/influxdb
		docker pull docker.io/google/cadvisor 
		docker  pull docker.io/grafana/grafana
		
		docker run -d -p 8086:8086 -v ~/influxdb:/var/libinfluxdb \
				--name influxdb tutum/influxdb
		docker  exec -ti influxdb influx
			create database "test"
			create user “root” with password ’password' with all privileges
			quit
		docker run -d \
				 -v /:/rootfs -v /var/run:/var/run -v /sys:/sys \
				 -v /var/lib/docker:/var/lib/docker \
				 --privileged=true \
				 --volume=/cgroup:/cgroup:ro \
				 --link=influxdb:influxdb --name cadvisor google/cadvisor \
				 --storage_driver=influxdb \
				 --storage_driver_host=influxdb:8086 \
				 --storage_driver_db=test \
				 --storage_driver_user=root \
				 --storage_driver_password=password \
				
		docker run -d -p 5000:3000 \
				-v ~/grafana:/var/lib/grafana \
				--link=influxdb:influxdb \
				--name grafana grafana/grafana
				
	
		
ETCD
    etcd -static
          https://discovery.etcd.io/new?3 --3个客户的
          http://discovery.etcd.io/7e4a9d839b4de8d66d47451c01740335       
            etcd --name infra0 \
                --initial-advertise-peer-urls http://192.168.1.30:2380 \
                --listen-peer-urls http://192.168.1.30:2380 \
                --listen-client-urls http://192.168.1.30:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://192.168.1.30:2379 \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster infra0=http://192.168.1.30:2380,infra1=http://192.168.1.31:2380,infra2=http://192.168.1.32:2380 \
                --initial-cluster-state new \
                --data-dir /root/etcddir &

              etcd --name infra1 \
                --initial-advertise-peer-urls http://192.168.1.31:2380 \
                --listen-peer-urls http://192.168.1.31:2380 \
                --listen-client-urls http://192.168.1.31:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://192.168.1.31:2379 \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster infra0=http://192.168.1.30:2380,infra1=http://192.168.1.31:2380,infra2=http://192.168.1.32:2380 \
                --initial-cluster-state new \
                --data-dir /root/etcddir &

              etcd --name infra2  \
                --initial-advertise-peer-urls http://192.168.1.32:2380 \
                --listen-peer-urls http://192.168.1.32:2380 \
                --listen-client-urls http://192.168.1.32:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://192.168.1.32:2379 \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster infra0=http://192.168.1.30:2380,infra1=http://192.168.1.31:2380,infra2=http://192.168.1.32:2380 \
                --initial-cluster-state new \
                --data-dir /root/etcddir &

  	etcd-discovery
            https://discovery.etcd.io/new?3 --3个客户的
            http://discovery.etcd.io/7e4a9d839b4de8d66d47451c01740335

              etcd --name infra0 \
                --initial-advertise-peer-urls http://192.168.1.30:2380 \
                --listen-peer-urls http://192.168.1.30:2380 \
                --listen-client-urls http://192.168.1.30:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://192.168.1.30:2379 \
                --data-dir /root/etcddir \
                --discovery http://discovery.etcd.io/7e4a9d839b4de8d66d47451c01740335 &

              etcd --name infra1 \
                --initial-advertise-peer-urls http://192.168.1.31:2380 \
                --listen-peer-urls http://192.168.1.31:2380 \
                --listen-client-urls http://192.168.1.31:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://192.168.1.31:2379 \
                --data-dir /root/etcddir \
                --discovery http://discovery.etcd.io/7e4a9d839b4de8d66d47451c01740335 &

              etcd --name infra2  \
                --initial-advertise-peer-urls http://192.168.1.32:2380 \
                --listen-peer-urls http://192.168.1.32:2380 \
                --listen-client-urls http://192.168.1.32:2379,http://127.0.0.1:2379 \
                --advertise-client-urls http://192.168.1.32:2379 \
                --data-dir /root/etcddir \
                --discovery http://discovery.etcd.io/7e4a9d839b4de8d66d47451c01740335 &

    etcdctl  member list
    etcdctl cluster-health
    etcdctl set 
    etcdctl get
    etcdctl update
    etcdctl rm
    etcdctl ls -r /
    etcdctl mkdir 

ETCD集群搭建配置                                                                  
      安装etcd服务                                                                                                                      
          # yum -y install etcd
          # cp /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak_$(date +%Y%m%d)
          # vim /etc/etcd/etcd.conf
                  ETCD_NAME=etcd_node1  // 节点名称
                  ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
                  ETCD_LISTEN_PEER_URLS="http://192.168.100.110:2380"
                  ETCD_LISTEN_CLIENT_URLS="http://192.168.100.110:2379,http://127.0.0.1:2379"  // 必须增加127.0.0.1否则启动会报错
                  ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.100.110:2380"
                  ETCD_INITIAL_CLUSTER="etcd_node1=http://192.168.100.110:2380,etcd_node2=http://192.168.100.111:2380"  // 集群IP地址
                  ETCD_INITIAL_CLUSTER_STATE="new"
                  ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
                  ETCD_ADVERTISE_CLIENT_URLS="http://192.168.100.110:2379"
          # systemctl enable etcd.service 
          # systemctl start etcd.service && systemctl status etcd.service
      验证etcd集群配置                                                                                                              
          # etcdctl cluster-health
              member 7e218077496bccf9 is healthy: got healthy result from http://localhost:2379
          cluster is healthy //表示安装成功 
                
        [root@centos73 etcd]# vim etcd.conf
                    # [member]
                    ETCD_NAME=c73
                    ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
                    #ETCD_WAL_DIR=""
                    #ETCD_SNAPSHOT_COUNT="10000"
                    #ETCD_HEARTBEAT_INTERVAL="100"
                    #ETCD_ELECTION_TIMEOUT="1000"
                    ETCD_LISTEN_PEER_URLS="http://192.168.1.30:2380"
                    ETCD_LISTEN_CLIENT_URLS="http://192.168.1.30:2379,http://127.0.0.1:2379"
                    #ETCD_MAX_SNAPSHOTS="5"
                    #ETCD_MAX_WALS="5"
                    #ETCD_CORS=""
                    #
                    #[cluster]
                    ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.1.30:2380"
                    # if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://...
                    "
                    ETCD_INITIAL_CLUSTER="c73=http://192.168.1.30:2380,c731=http://192.168.1.31:2380,c732=http://192.168.1.32:2380"
                    ETCD_INITIAL_CLUSTER_STATE="new"
                    ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-1"
                    ETCD_ADVERTISE_CLIENT_URLS="http://192.168.1.30:2379"
                    #ETCD_DISCOVERY=""
                    #ETCD_DISCOVERY_SRV=""
                    #ETCD_DISCOVERY_FALLBACK="proxy"
                    #ETCD_DISCOVERY_PROXY=""
                    #ETCD_STRICT_RECONFIG_CHECK="false"
                    #ETCD_AUTO_COMPACTION_RETENTION="0"
                    #
                    #[proxy]
                    #ETCD_PROXY="off"
                    #ETCD_PROXY_FAILURE_WAIT="5000"
                    "etcd.conf" 55L, 1611C                                                                         1,1           Top
                    # [member]
                    ETCD_NAME=c73
                    ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
                    #ETCD_WAL_DIR=""
                    #ETCD_SNAPSHOT_COUNT="10000"
                    #ETCD_HEARTBEAT_INTERVAL="100"
                    #ETCD_ELECTION_TIMEOUT="1000"
                    ETCD_LISTEN_PEER_URLS="http://192.168.1.30:2380"
                    ETCD_LISTEN_CLIENT_URLS="http://192.168.1.30:2379,http://127.0.0.1:2379"
                    #ETCD_MAX_SNAPSHOTS="5"
                    #ETCD_MAX_WALS="5"
                    #ETCD_CORS=""
                    #
                    #[cluster]
                    ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.1.30:2380"
                    # if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://...
                    "
                    ETCD_INITIAL_CLUSTER="c73=http://192.168.1.30:2380,c731=http://192.168.1.31:2380,c732=http://192.168.1.32:2380"
                    ETCD_INITIAL_CLUSTER_STATE="new"
                    ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-1"
                    ETCD_ADVERTISE_CLIENT_URLS="http://192.168.1.30:2379"
                    #ETCD_DISCOVERY=""
                    #ETCD_DISCOVERY_SRV=""
                    #ETCD_DISCOVERY_FALLBACK="proxy"
                    #ETCD_DISCOVERY_PROXY=""
                    #ETCD_STRICT_RECONFIG_CHECK="false"
                    #ETCD_AUTO_COMPACTION_RETENTION="0"
                    #
                    #[proxy]
                    #ETCD_PROXY="off"
                    #ETCD_PROXY_FAILURE_WAIT="5000"
        
    vim  /usr/lib/systemd/system/etcd.service
                    "etcd.service" 18L, 762C                                                                       13,1          All
                    [Unit]
                    Description=Etcd Server
                    After=network.target
                    After=network-online.target
                    Wants=network-online.target

                    [Service]
                    Type=notify
                    WorkingDirectory=/var/lib/etcd/
                    EnvironmentFile=-/etc/etcd/etcd.conf
                    User=etcd
                    # set GOMAXPROCS to number of processors
                    ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\"${ETCD_NAME}\" --data-dir=\"${ETCD_DATA_DIR}\"
                     --listen-client-urls=\"${ETCD_LISTEN_CLIENT_URLS}\" --listen-peer-urls=\"${ETCD_LISTEN_PEER_URLS}\" --advertise-
                    client-urls=\"${ETCD_ADVERTISE_CLIENT_URLS}\" --initial-cluster-token=\"${ETCD_INITIAL_CLUSTER_TOKEN}\" --initial
                    -cluster=\"${ETCD_INITIAL_CLUSTER}\" --initial-cluster-state=\"${ETCD_INITIAL_CLUSTER_STATE}\" "
                    Restart=on-failure
                    LimitNOFILE=65536

                    [Install]
                    WantedBy=multi-user.target
    
    
 docer:   docker.io/elcolio/etcd:latest
 
        1     https://discovery.etcd.io/90c99883cdce20a7420f964f2234cfec \
              
              docker run \
              -d \
              -p 2379:2379 \
              -p 2380:2380 \
              -p 4001:4001 \
              -p 7001:7001 \
              -v /data/backup/dir:/data \
              --name some-etcd \
              elcolio/etcd:latest \
              -name some-etcd \
              -discovery=https://discovery.etcd.io/90c99883cdce20a7420f964f2234cfec \
              -advertise-client-urls http://192.168.1.30:4001 \
              -initial-advertise-peer-urls http://192.168.1.30:7001
                  
        2     docker run \
              -d \
              -p 2379:2379 \
              -p 2380:2380 \
              -p 4001:4001 \
              -p 7001:7001 \
              -v /data/backup/dir:/data \
              --name some-etcd \
              elcolio/etcd:latest \
              -name some-etcd1 \
              -discovery=https://discovery.etcd.io/90c99883cdce20a7420f964f2234cfec \
              -advertise-client-urls http://192.168.1.31:4001 \
              -initial-advertise-peer-urls http://192.168.1.31:7001
              
        3       docker run \
              -d \
              -p 2379:2379 \
              -p 2380:2380 \
              -p 4001:4001 \
              -p 7001:7001 \
              -v /data/backup/dir:/data \
              --name some-etcd \
              elcolio/etcd:latest \
              -name some-etcd2 \
              -discovery=https://discovery.etcd.io/90c99883cdce20a7420f964f2234cfec \
              -advertise-client-urls http://192.168.1.32:4001 \
              -initial-advertise-peer-urls http://192.168.1.32:7001
    
    curl -L http://172.17.8.101:2379/version  --测试 etcd
        
24  flannel
        # flanneld --help
            Usage: flanneld [OPTION]...
                    -etcd-endpoints string
                        a comma-delimited list of etcd endpoints (default "http://127.0.0.1:2379")
        1.  yum install  flannel
                    /etc/sysconfig/flanneld --flanneld 配置文件
        
        2.  etcdctl set  /atomic.io/network/config '{Network":"172.17.0.0/16"}'  --设置网络地址
        3.  systemctl  start  flanneld   -启动flannel
             flanneld-start   -与上相同
            flanned -etcd-endpoints  =http://192.168.x.x:2379    
        4.  sh ip addr   --检查是否有 flannel0 的设备
            source /run/flannel/subnet
        5.  /usr/libexec/flannel/mk-docker-opts.sh -生成docker 启动参数
        6.  /run/flannel/docker       -生成docker 参数
            /run/flannel/subnet.env
            /run/docker_opts.env -将此文件内容附加到 /etc/sysconfig/docker  的DOCKER 运行参数后面。
            
            ifconfig docker0 ${FLANNEL_SUBNET} --（可不要执行）。
        
        7.  /etc/sysconfig/docker        -docker 启动参数
            /etc/sysconfig/docker_network        - 网络参数   将生成的参数加入到的文件中
        8.  systemctl  restart  docker     -重启docker
        9.  sh ip add   --检查docker0 的网络是否与flannel0 设置的范围相同。
        10  etcdctl  ls /atomic.io/network/subnets
        11  etcdctl  get /automic.io/network/subnets/172.17.x.x-24
    cbcv    
    
calico
        1.  curl -L http://172.17.8.101:2379/version --测试etcd
       
        2   vim  /etc/docker/daemon.json
                "cluster-store": "etcd://192.168.1.30:2379"
        3.  systemctl daemon-reload
        4.  systemctl  restart  docker
        5.  calicoctl get  node
        6.  calicoctl get ipPool
        6.  calicoctl node  run  --ip=192.168.1.0
        7.  calicoctl node status
        
     1.         vim  /root/ipPool.yaml
                      - apiVersion: v1
                      kind: ipPool
                      metadata:
                        cidr: 10.20.0.0/24
                      spec:
                        ipip:
                          enabled: true
                        nat-outgoing: true
                cat << EOF | calicoctl create -f -
                        - apiVersion: v1
                          kind: ipPool
                          metadata:
                            cidr: 192.0.2.0/24
                        EOF
                        
      calicoctl  create -f ippool.yaml
      calicoctl  get  ipPool
      calicoctl  get  workloadendpoint
      calicoctl get ipPool --output=wide
      
      calicoctl  node --ip=172.17.122.22  --设置node
          calicoctl node status
    
    
docker  批量删除 镜像仓库
        docker rmi $(docker images | grep "none" | awk '{print $3}') 
      
      docker network create --driver calico  --ipam-driver  calico-ipam  --subnet 192.168.x.x/24  mynet
        docker network ls
     
        calicoctl config set nodeTonodeMesh off  --关闭全互联模式
        --BGP Speaker RR模式，就是在网络中指定一个或多个BGP Speaker作为Router Reflection，RR与所有的BGP Speaker建立bgp连接。
                                                关闭了全互联模式后，再将RR作为Global Peers添加到Calico中，Calico网络就切换到了RR模式，可以支撑容纳更多的node。     
        $ calicoctl apply -f - << EOF
        apiVersion: v1
        kind: ipPool
        metadata:
          cidr: 172.16.0.0/16
        spec:
          ipip:
            enabled: true
            mode: always
          nat-outgoing: true
        EOF
       
kubernetes
0.          yum install etcd flanned ntpdate
                timedatectl   查看时区
                timedatectl set-timezone Asia/Shanghai
                ntpdate -u  time.nist.gov  时间同步
                hwclock -w   软件时间to硬件时钟
0.1         指定docer 镜像仓库

      cat > /etc/docker/daemon.json <<EOF
      {
      "registry-mirrors": ["https://9npjh5s8.mirror.aliyuncs.com"]
      }
      EOF
      vim /etc/docker/daemon.json
            {
              "registry-mirrors":  ["https://9npjh5s8.mirror.aliyuncs.com"]
            }

1   etcd
        https://discovery.etcd.io/new?size=5
        812490a4405328af0329f105628f8c09
        71：
        etcd -name infra1 -initial-advertise-peer-urls http://192.168.110.71:2380 -listen-peer-urls http://192.168.110.71:2380 -listen-client-urls http://192.168.110.71:2379,http://127.0.0.1:2379 -advertise-client-urls http://192.168.110.71:2379  -discovery https://discovery.etcd.io/812490a4405328af0329f105628f8c09 --data-dir /usr/local/kubernete_test/flanneldata  >> /usr/local/kubernete_test/logs/etcd.log 2>&1 &
        72：
        etcd -name infra2 -initial-advertise-peer-urls http://192.168.110.72:2380 -listen-peer-urls http://192.168.110.72:2380 -listen-client-urls http://192.168.110.72:2379,http://127.0.0.1:2379 -advertise-client-urls http://192.168.110.72:2379  -discovery https://discovery.etcd.io/812490a4405328af0329f105628f8c09 --data-dir /usr/local/kubernete_test/flanneldata  >> /usr/local/kubernete_test/logs/etcd.log 2>&1 &
        73
        etcd -name infra3 -initial-advertise-peer-urls http://192.168.110.73:2380 -listen-peer-urls http://192.168.110.73:2380 -listen-client-urls http://192.168.110.73:2379,http://127.0.0.1:2379 -advertise-client-urls http://192.168.110.73:2379  -discovery https://discovery.etcd.io/812490a4405328af0329f105628f8c09 --data-dir /usr/local/kubernete_test/flanneldata  >> /usr/local/kubernete_test/logs/etcd.log 2>&1 &
        74
        etcd -name infra4 -initial-advertise-peer-urls http://192.168.110.74:2380 -listen-peer-urls http://192.168.110.74:2380 -listen-client-urls http://192.168.110.74:2379,http://127.0.0.1:2379 -advertise-client-urls http://192.168.110.74:2379  -discovery https://discovery.etcd.io/812490a4405328af0329f105628f8c09 --data-dir /usr/local/kubernete_test/flanneldata  >> /usr/local/kubernete_test/logs/etcd.log 2>&1 &

2     etcdctl  set  /atomic.io/network/config '{ "Network": "172.17.0.0/16" }'
3     flanneld >> /usr/local/kubernete_test/logs/flanneld.log 2>&1 &
      source /run/flannel/subnet.env 
      /usr/libexec/flannel/mk-docker-opts.sh  -i  （生成 /run/docker_opts.env 运行参数文件）
4     vim  /etc/sysconfig/docker
            插入文件 /run/docker_opts.env 更改 运行参数
5     systemctl  daemon-reload
      systemctl  restart docker
6     ip add sh  检查一下docker0 的ip 与flannel0 的IP 地址是否一致
  
      master：     yum install -y  kubernetes-master
      client:     yum install -y  kubernetes-node

      cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
      [kubernetes]
      name=Kubernetes
      baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
      enabled=1
      gpgcheck=0
      EOF 
    
    master:
       vim /etc/kubernetes
          vim  config 
          vim  apiserver  
            ###
            # kubernetes system config
            #
            # The following values are used to configure the kube-apiserver
            #

            # The address on the local server to listen to.
            KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"
            # The port on the local server to listen on.
              KUBE_API_PORT="--port=8080"
            # Port minions listen on
            #KUBELET_PORT="--kubelet-port=10250"
            # Comma separated list of nodes in the etcd cluster
            KUBE_ETCD_SERVERS="--etcd-servers=http://192.168.110.71:2379,http://192.168.110.73:2379,http://192.168.110.74:2379"
            # Address range to use for services
      vim  /etc/kubernetes/apiserver
            # kubernetes system config
            #
            # The following values are used to configure the kube-apiserver
            #
            # The address on the local server to listen to.
            KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"
            # The port on the local server to listen on.
              KUBE_API_PORT="--port=8080"
            # Port minions listen on
              KUBELET_PORT="--kubelet-port=10250"
            # Comma separated list of nodes in the etcd cluster
            KUBE_ETCD_SERVERS="--etcd-servers=http://192.168.110.71:2379,http://192.168.110.73:2379,http://192.168.110.74:2379"
            # Address range to use for services
            KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"
            # default admission control policies
            KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota"
            # Add your own!
            KUBE_API_ARGS=""                                                             

        sytemctl daemon-reload
        systemctl start kube-apiserver
        systemctl start kube-controller-manager
        systemctl start kube-scheduler

        kubectl  get componentstatuses  （cs）
        kubectl  get nodes
        kubectl  get  podes
        kubectl  get  endpoints
        kubectl get deployment  (deploy)
        kubectl  describe node xxx
               
  client:
      vim /etc/kubernetes
          vim config
              # The following values are used to configure various aspects of all
              # kubernetes services, including
              #
              #   kube-apiserver.service
              #   kube-controller-manager.service
              #   kube-scheduler.service
              #   kubelet.service
              #   kube-proxy.service
              # logging to stderr means we get it in the systemd journal
              KUBE_LOGTOSTDERR="--logtostderr=true"

              # journal message level, 0 is debug
              KUBE_LOG_LEVEL="--v=0"

              # Should this cluster be allowed to run privileged docker containers
              KUBE_ALLOW_PRIV="--allow-privileged=false"

              # How the controller-manager, scheduler, and proxy find the apiserver
              KUBE_MASTER="--master=http://192.168.110.71:8080"
          vim kubelet
                  # kubernetes kubelet (minion) config
                  # The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
                  KUBELET_ADDRESS="--address=192.168.110.73"
                  # The port for the info server to serve on
                  # KUBELET_PORT="--port=10250"
                  # You may leave this blank to use the actual hostname
                  KUBELET_HOSTNAME="--hostname-override=192.168.110.73"
                  # location of the api-server
                  KUBELET_API_SERVER="--api-servers=http://192.168.110.71:8080"
                  # pod infrastructure container
                    KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"
                  # Add your own!
                  KUBELET_ARGS=""
          vim kube-proxy
            
    kubectl  run nginx  --image=nginx  --port=80 --replicas=5 
    kubectl  expose deployment nginx --type=NodePort --name=nginx-service --external-ip=192.168.110.73
    kubectl --server=192.168.110.71:80 get service nginx-service
    kubectl --server=192.168.110.71:80 describe service nginx-service
  测试：
    在client 是可以访问 10.254.x.x:80    
                curl   10.254.x.x:80    
                
        curl  192.168.110.73:80
        curl  172.19.x.x:80
        在外网访问  external-ip 地址   
            curl  192.168.110.73:80
        
    kube-apiserver --address=0.0.0.0  --insecure-port=8080 --service-cluster-ip-range='10.254.0.0/16' --log_dir=/usr/local/kubernete_test/logs/kube --kubelet_port=10250 --v=0 --logtostderr=false --etcd_servers=http://192.168.110.71:2379 --allow_privileged=false  >> /usr/local/kubernete_test/logs/kube-apiserver.log 2>&1 &
    kube-controller-manager  --v=0 --logtostderr=false --log_dir=/usr/local/kubernete_test/logs/kube --master=192.168.110.71:8080 >> /usr/local/kubernete_test/logs/kube-controller-manager 2>&1 &
    kube-scheduler  --master='192.168.110.71:8080' --v=0  --log_dir=/usr/local/kubernete_test/logs/kube  >> /usr/local/kubernete_test/logs/kube-scheduler.log 2>&1 &
   
kubeadm:
    docker pull  index.tenxcloud.com/jimmy/elasticsearch:v2.4.1-2
    docker pull   index.tenxcloud.com/jimmy/fluentd-elasticsearch:1.22
    docker pull  index.tenxcloud.com/jimmy/kibana:v4.6.1-1
    docker pull  index.tenxcloud.com/jimmy/heapster-grafana-amd64:v4.0.2
    docker pull  index.tenxcloud.com/jimmy/heapster-amd64:v1.3.0-beta.1
    docker pull  index.tenxcloud.com/jimmy/heapster-influxdb-amd64:v1.1.1
    docker pull  index.tenxcloud.com/jimmy/kubernetes-dashboard-amd64:v1.6.0
    docker pull  index.tenxcloud.com/jimmy/k8s-dns-kube-dns-amd64:1.14.1
    docker pull  index.tenxcloud.com/jimmy/k8s-dns-dnsmasq-nanny-amd64:1.14.1
    docker pull  index.tenxcloud.com/jimmy/k8s-dns-sidecar-amd64:1.14.1
    docker pull 4admin2root/kube-controller-manager-amd64:v1.6.0
    docker pull 4admin2root/kube-scheduler-amd64:v1.6.0
    docker pull 4admin2root/kube-apiserver-amd64:v1.6.0
    docker pull 4admin2root/etcd-amd64:3.0.17
    docker pull 4admin2root/kube-proxy-amd64:v1.6.0
    docker pull  4admin2root/k8s-dns-sidecar-amd64:1.14.1
    docker pull  4admin2root/k8s-dns-dnsmasq-nanny-amd64:1.14.1
    docker pull  4admin2root/pause-amd64:3.0
    docker pull 4admin2root/etcd:2.2.1
    docker pull 4admin2root/node:v1.1.0
    docker pull 4admin2root/cni:v1.6.1
    docker pull 4admin2root/kube-policy-controller:v0.5.4

  CentOS 7  yum 源
    wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
    或者
    curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
    3、之后运行yum makecache生成缓存
      使用说明
      首先备份/etc/yum.repos.d/CentOS-Base.repo
      mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
      下载对应版本repo文件, 放入/etc/yum.repos.d/(操作前请做好相应备份)
          CentOS7
          CentOS6
          CentOS5
      运行以下命令生成缓存
          yum clean all
          yum makecache
            
kubernetes 1.5.2 yum源
        [virt7-docker-common-candidate]
        name=virt7-docker-common-candidate
        baseurl=https://cbs.centos.org/repos/virt7-docker-common-candidate/x86_64/os/
        enabled=1
        gpgcheck=0
        EOF

docker-ce  安装
    CentOS 7 (使用yum进行安装)
        # step 1: 安装必要的一些系统工具
        sudo yum install -y yum-utils device-mapper-persistent-data lvm2
        # Step 2: 添加软件源信息
        sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
        # Step 3: 更新并安装 Docker-CE
        sudo yum makecache fast
        sudo yum -y install docker-ce
        # Step 4: 开启Docker服务
        sudo service docker start

1. yum  源
        tee /etc/yum.repos.d/kubernetes.repo << EOF
          [kubernetes]
          name=kubernetes
          baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
          enabled=1
          gpgcheck=0
          EOF
      
          #kubernetes yum源
          cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
          [kubernetes]
          name=Kubernetes
          baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
          enabled=1
          gpgcheck=0
          EOF
      
      tee /etc/yum.repos.d/docker.repo << EOF
          [dockerrepo]
          name=Docker Repository
          baseurl=https://yum.dockerproject.org/repo/main/centos/7/
          enabled=1
          gpgcheck=1
          gpgkey=https://yum.dockerproject.org/gpg    
          EOF
      
      cat >> /etc/yum.repos.d/docker.repo <<EOF
          [docker-repo]
          name=Docker Repository
          baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7
          enabled=1
          gpgcheck=0
          EOF
      
2.  yum clean all
    yum makecache

    yum list | grep docker | sort -r    
    yum  list  installed  | grep docker*
        
3. 开启路由转发
        echo 1 > /proc/sys/net/ipv4/ip_forward
        echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
        echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
        sysctl -p
禁用 swapp
      swapoff  -a
        
4. 禁用ipv6
        lsmod | grep -i ipv6
        ifconfig | grep -i inet6
            
5. 清空iptables
        iptables -X
        iptables -Z
        iptables -P INPUT ACCEPT
        iptables -F
            
6.  关闭防火墙
        setenforce=0
        systemctl disable firewalld.service
        systemctl stop firewalld.service

5. 安装Docker
    yum install -y docker
    systemctl enable docker
    systemctl start docker

6.  Docker,加速器，避免自己下载镜像速度太慢
    修改/etc/docker/daemon.json 添加如下内容：
        cat > /etc/docker/daemon.json <<EOF
  {
  "registry-mirrors": ["https://9npjh5s8.mirror.aliyuncs.com"]
  }
  EOF
        systemctl daemon-reload
        systemctl enable docker
        systemctl start docker      
    
安装 kubeadm
     yum install -y  kubelet kubeadm kubectl kubernetes-cni
		 systemctl  eanble  kubelet
		 systemctl  start  kubelet
		 
		  
	下载镜像
		cat > k8s.sh <<EOF
		#!/bin/bash
		images=(kube-proxy-amd64:v1.10.0 kube-scheduler-amd64:v1.10.0 kube-controller-manager-amd64:v1.10.0 kube-apiserver-amd64:v1.10.0
		etcd-amd64:3.1.12 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.8 k8s-dns-kube-dns-amd64:1.14.8
		k8s-dns-dnsmasq-nanny-amd64:1.14.8)
		for imageName in ${images[@]} ; do
		  docker pull mirrorgooglecontainers/$imageName
		  docker tag mirrorgooglecontainers/$imageName k8s.gcr.io/$imageName
		  docker rmi mirrorgooglecontainers/$imageName
		done
		EOF
		
		chmod +x  k8s
		./k8s.sh    #运行脚本， 下载镜像
			
	安装kubernetes
	
		  kubeadm init --kubernetes-version=v1.10.0 --pod-network-cidr=10.244.0.0/16
         
          kubeadm  reset  - 删除k8s
		  $ kubeadm reset
			$ ifconfig cni0 down && ip link delete cni0
			$ ifconfig flannel.1 down && ip link delete flannel.1
			$ rm -rf /var/lib/cni/
	配置环境 
		    mkdir -p $HOME/.kube
			sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
			sudo chown $(id -u):$(id -g) $HOME/.kube/config
			
			把master节点的~/.kube/config文件拷贝到当前节点对应的位置即可使用kubectl命令行工具了。
			
		五、在master节点上部署网络插件flannel

        wget https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
        kubectl create -f kube-flannel.yml
        
		
          kubectl --kubeconfig /etc/kubernetes/admin.conf get pod --all-namespaces /
          
          
          kubectl apply -f http://docs.projectcalico.org/v2.1/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml --kubeconfig /etc/kubernetes/admin.conf
          kubectl apply -f http://docs.projectcalico.org/v2.1/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml

          kubectl apply -f http://k8s.oss-cn-shanghai.aliyuncs.com/kube/kubernetes-dashboard1.5.0.yaml
          
          kubectl --kubeconfig /etc/kubernetes/kubelet.conf get pod --all-namespaces /
		  
		  
		  --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice


    mirrorgooglecontainers/kube-proxy-amd64		  
          
      #!/bin/bash
      images=(kube-proxy-amd64:v1.10.0 kube-scheduler-amd64:v1.10.0 kube-controller-manager-amd64:v1.10.0 kube-apiserver-amd64:v1.10.0
      etcd-amd64:3.1.12 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.8 k8s-dns-kube-dns-amd64:1.14.8
      k8s-dns-dnsmasq-nanny-amd64:1.14.8)
      for imageName in ${images[@]} ; do
        docker pull mirrorgooglecontainers/$imageName
        docker tag mirrorgooglecontainers/$imageName k8s.gcr.io/$imageName
        docker rmi mirrorgooglecontainers/$imageName
      done
keveon/$imageName
    docker  安装： 
        yum  install -y  docker-engin
        systemctl start docker
        systemctl disable  firewalld
        yum install -y iptables-services
        systemctl enable iptables
        systemctl start iptables

    #!/bin/bash
    set -o errexit
    set -o nounset
    set -o pipefail

    KUBE_VERSION=v1.7.2
    KUBE_PAUSE_VERSION=3.0
    ETCD_VERSION=3.0.17
    DNS_VERSION=1.14.4

    GCR_URL=gcr.io/google_containers
    ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/szss_k8s

    images=(kube-proxy-amd64:${KUBE_VERSION}
    kube-scheduler-amd64:${KUBE_VERSION}
    kube-controller-manager-amd64:${KUBE_VERSION}
    kube-apiserver-amd64:${KUBE_VERSION}
    pause-amd64:${KUBE_PAUSE_VERSION}
    etcd-amd64:${ETCD_VERSION}
    k8s-dns-sidecar-amd64:${DNS_VERSION}
    k8s-dns-kube-dns-amd64:${DNS_VERSION}
    k8s-dns-dnsmasq-nanny-amd64:${DNS_VERSION})

    for imageName in ${images[@]} ; do
      docker pull $ALIYUN_URL/$imageName
      docker tag $ALIYUN_URL/$imageName $GCR_URL/$imageName
    # docker push $ALIYUN_URL/$imageName
      docker rmi $ALIYUN_URL/$imageName 
    done

    cat > /etc/systemd/system/kubelet.service.d/20-pod-infra-image.conf <<EOF
    [Service]
    Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/szss_k8s/pause-amd64:3.0"
    EOF

  sed -i.bak 's/cgroup-driver=systemd/cgroup-driver=cgroupfs/g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

  export KUBE_REPO_PREFIX="registry.cn-hangzhou.aliyuncs.com/szss_k8s"
  export KUBE_ETCD_IMAGE="registry.cn-hangzhou.aliyuncs.com/szss_k8s/etcd-amd64:3.0.17"

  echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables

  kubectl --namespace kube-system apply -f https://raw.githubusercontent.com/coreos/flannel/v0.8.0/Documentation/kube-flannel-rbac.yml

  sed -i 's/quay.io\/coreos\/flannel:v0.8.0-amd64/registry.cn-hangzhou.aliyuncs.com\/szss_k8s\/flannel:v0.8.0-amd64/g' ./kube-flannel.yml
  kubectl --namespace kube-system apply -f ./kube-flannel.yml
 
NODE INSTALL 
 http://blog.csdn.net/running_free/article/details/78398984

systemctl disable firewalld
systemctl stop firewalld
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
iptables -F
setenforce 0 
 
##docker yum源

 增加docker repository
        yum-config-manager \
            --add-repo \
                    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
        # 下载包信息到本地
        yum makecache fast

yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

##kubernetes yum源
cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
EOF


cat >> /etc/yum.repos.d/docker.repo <<EOF
[docker-repo]
name=Docker Repository
baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7
enabled=1
gpgcheck=0
EOF


vi /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1


 docker安装
    Kubernetes 1.6还没有针对docker 1.13和最新的docker 17.03上做测试和验证，所以这里安装Kubernetes官方推荐的Docker 1.12版本。

[root@server2 ~]# yum list docker-engine --showduplicates ##查看docker版本
[root@server2 ~]# yum install -y docker-engine-1.12.6-1.el7.centos.x86_64  ##安装docker


直接这样装会报错。需要同时安装docker-engine-selinux-1.12.6-1.el7.centos.noarch.rpm，如下

[root@server2 ~]# yum install -y docker-engine-1.12.6-1.el7.centos.x86_64 docker-engine-selinux-1.12.6-1.el7.centos.noarch

##或者将这两个包下载下来，再一起安装

[root@server2 ~]# wget https://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/Packages/docker-engine-1.12.6-1.el7.centos.x86_64.rpm
[root@server2 ~]# wget https://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/Packages/docker-engine-selinux-1.12.6-1.el7.centos.noarch.rpm
[root@server2 ~]# yum install -y docker-engine-1.12.6-1.el7.centos.x86_64.rpm docker-engine-selinux-1.12.6-1.el7.centos.noarch.rpm

  
    kubernetes安装：

[root@server2 ~]# yum list kubeadm --showduplicates
[root@server2 ~]# yum list kubernetes-cni --showduplicates
[root@server2 ~]# yum list kubelet --showduplicates
[root@server2 ~]# yum list kubectl --showduplicates
##以上为查看可用版本，选择合适版本安装即可

[root@server2 ~]# yum install -y kubernetes-cni-0.5.1-0.x86_64 kubelet-1.7.2-0.x86_64 kubectl-1.7.2-0.x86_64 kubeadm-1.7.2-0.x86_64
export KUBE_ETCD_IMAGE="registry.cn-hangzhou.aliyuncs.com/szss_k8s/etcd-amd64:3.0.17"
export KUBE_REPO_PREFIX="registry.cn-hangzhou.aliyuncs.com/szss_k8s"


sed -i.bak 's/cgroup-driver=systemd/cgroup-driver=cgroupfs/g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

cat > /etc/systemd/system/kubelet.service.d/20-pod-infra-image.conf <<EOF
[Service]
Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/szss_k8s/pause-amd64:3.0"
EOF

检查 DNS :

1. vim busybox.yml
apiVersion: v1
kind: Pod
metadata:
    name: busybox
    namespace: default
spec:
    containers:
      - image: busybox
        command:
          - sleep
          - "3600"
        imagePullPolicy: IfNotPresent
        name: busybox
    restartPolicy: Always
    
2.kubectl create -f busybox.yaml
      pod "busybox" created

3. kubectl exec busybox -- nslookup kubernetes
        Server:    10.96.0.10
        Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
        Name:      kubernetes
        Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local

测试 应用实例
1. vim rc-nginx.yaml 
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-nginx-2
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx-2
    spec:
      containers:
      - name: nginx-2
        image: docker.io/nginx
        ports:
        - containerPort: 80
2. kubectl create -f rc-nginx.yaml
        replicationcontroller "rc-nginx-2" created
3. kubectl get rc
        NAME         DESIRED   CURRENT   READY     AGE
        rc-nginx-2   2         2         0         12s

wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  
kubectl  apply -f  kube-flannel.yml


kuectl get pods --all-namespaces  -o wide      
        
dashboard

# openssl req -newkey rsa:4096 -nodes -sha256 -keyout alleyz.key -x509 -days 365 -out dashboard.crt

==========================================================================================================================
cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
EOF


cat >> /etc/yum.repos.d/docker.repo <<EOF
[docker-repo]
name=Docker Repository
baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7
enabled=1
gpgcheck=0
EOF


yum install -y yum-utils device-mapper-persistent-data lvm2

yum list docker-engine --showduplicates

yum  list  xxx     --show-duplicates, --showduplicates

yum install docker-engine-1.12.6-1.el7.centos  -y docker-engine-selinux-1.12.6-1.el7.centos
yum  install kubeadm kubectl kubelet kubernetes-cni -y


systemctl daemon-reload
echo 1 > /proc/sys/net/ipv4/ip_forward
echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
echo 1 > /proc/sys/net/bridge/bridge-nf-call-ip6tables
sysctl -p
systemctl restart kubelet
systemctl restart docker


 cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
 sudo sysctl --system
 
 

vim /etc/docker/daemon.json
{
"registry-mirrors": ["https://9npjh5s8.mirror.aliyuncs.com"]
}


vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
   更改  cgroupfs



kubeadm init --kubernetes-version=v1.6.13 --pod-network-cidr=10.244.0.0/16

# 对于非root用户
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 对于root用户
$ export KUBECONFIG=/etc/kubernetes/admin.conf
# 也可以直接放到~/.bash_profile
$ echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
默认情况下，为了保证master的安全，master是不会被调度到app的。你可以取消这个限制通过输入：

$ kubectl taint nodes --all node-role.kubernetes.io/master-


wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  

kubectl apply  -f  kube-flannel.yml

https://github.com/rootsongjc/follow-me-install-kubernetes-cluster/tree/master/manifests/dashboard
kube-dashboard


    dashboard-contoller.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kubernetes-dashboard
  namespace: kube-system
  labels:
    k8s-app: kubernetes-dashboard
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      serviceAccountName: dashboard
      containers:
      - name: kubernetes-dashboard
        image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.0
        resources:
          # keep request = limit to keep this container in guaranteed class
          limits:
            cpu: 100m
            memory: 50Mi
          requests:
            cpu: 100m
            memory: 50Mi
        ports:
        - containerPort: 9090
        livenessProbe:
          httpGet:
            path: /
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
      tolerations:
      - key: "CriticalAddonsOnly"
        operator: "Exists"


    dashboard-rbac.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard
  namespace: kube-system

---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1alpha1
metadata:
  name: dashboard
subjects:
  - kind: ServiceAccount
    name: dashboard
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io



    dashboard-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: kubernetes-dashboard
  namespace: kube-system
  labels:
    k8s-app: kubernetes-dashboard
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  type: NodePort 
  selector:
    k8s-app: kubernetes-dashboard
  ports:
  - port: 80
    targetPort: 9090
    nodePort: 38888


1.1 方案1:使用阿里云yum镜像

配置yum源，由于google被墙，可以使用阿里云搭建的yum源

#docker yum源
cat >> /etc/yum.repos.d/docker.repo <<EOF
[docker-repo]
name=Docker Repository
baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7
enabled=1
gpgcheck=0
EOF

#kubernetes yum源
cat >> /etc/yum.repos.d/kubernetes.repo <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
EOF

批量删除镜像：

docker images | grep dns  | awk '{print $3}'
docker rmi $(docker images | grep dns  | awk '{print $3}')




   
31.gitlab
		docker pull gitlab/gitlab-ce  
		wget https://raw.githubusercontent.com/sameersbn/docker-gitlab/master/docker-compose.yml
		docker-compose up

			Start GitLab using:

			docker-compose up
			Alternatively, you can manually launch the gitlab container and the supporting postgresql and redis containers by following this three step guide.

			Step 1. Launch a postgresql container

			docker run --name gitlab-postgresql -d \
				--env 'DB_NAME=gitlabhq_production' \
				--env 'DB_USER=gitlab' --env 'DB_PASS=password' \
				--env 'DB_EXTENSION=pg_trgm' \
				--volume /srv/docker/gitlab/postgresql:/var/lib/postgresql \
				sameersbn/postgresql:9.6-2
			Step 2. Launch a redis container

			docker run --name gitlab-redis -d \
				--volume /srv/docker/gitlab/redis:/var/lib/redis \
				sameersbn/redis:latest
			Step 3. Launch the gitlab container

			docker run --name gitlab -d \
				--link gitlab-postgresql:postgresql --link gitlab-redis:redisio \
				--publish 10022:22 --publish 10080:80 \
				--env 'GITLAB_PORT=10080' --env 'GITLAB_SSH_PORT=10022' \
				--env 'GITLAB_SECRETS_DB_KEY_BASE=long-and-random-alpha-numeric-string' \
				--env 'GITLAB_SECRETS_SECRET_KEY_BASE=long-and-random-alpha-numeric-string' \
				--env 'GITLAB_SECRETS_OTP_KEY_BASE=long-and-random-alpha-numeric-string' \
				--volume /srv/docker/gitlab/gitlab:/home/git/data \
				sameersbn/gitlab:10.5.6
			Please refer to Available Configuration Parameters to understand GITLAB_PORT and other configuration options

			NOTE: Please allow a couple of minutes for the GitLab application to start.

			Point your browser to http://localhost:10080 and set a password for the root user account

32.	jenkins
		docker pull jenkins    
		chmod 777 -R  /root/jenkins
		docker run -p 8080:8080 -p 50000:50000 -v /root/jenkins:/var/jenkins_home jenkins

33. swarm
    docker swarm init --advertise-addr  192.168.1.12
    docker swarm join     --token SWMTKN-1-24uw3xrfo0for1gb6tvqn0vomlnp9vgr05kpe03p9iknfll4qo-cv1jgkz320ydu09flw990urec   192.168.1.12:237
    docker network create -d overlay  --subnet  10.25.0.0/24  overnet
    docker service create  --network overnet  nginx


34. docker yum 源设置------------------------------------------

      #docker yum源
      cat >> /etc/yum.repos.d/docker.repo <<EOF
      [docker-repo]
      name=Docker Repository
      baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
      enabled=1
      gpgcheck=0
      EOF

      cat >> /etc/yum.repos.d/docker-engine.repo <<'EOF'
      [dockerrepo]
      name=Docker Repository
      baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
      enabled=1
      gpgcheck=1
      gpgkey=https://yum.dockerproject.org/gpg
      EOF

35.docker--------influxdb+cadvisor+grafana--------------------------------------
      docker  监控：
      1.镜像准备
        docker.io/tutum/influxdb
        docker.io/google/cadvisor
        docker.io/grafana/grafana
      2. docker  run  -d -p 8086:8086 -v ~/influxdb:/var/lib/influxdb --name influxdb  tutum/influxdb
          docker exec -ti influxdb  influx  --进行influxdb 数据库
            create database 'test'
            create user "root" with password 'password' with all privileges
            quit
          docker  run -d -v /:/rootfs  -v /var/run:/var/run -v /sys:/sys \
            -v /var/lib/docker:/var/lib/docker  \
            --volume /cgroup:/cgroup:ro \
            --privileged=true  \
            --link=influxdb:influxdb --name cadvisor google/cadvisor \
            --storage_driver=influxdb  \
            --storage_driver_host=influxdb:8086 \
            --storage_driver_db=test \
            --storage_driver_user=root \
            --storage_driver_password=password
      3. docker run -d -p 5000:3000 -v ~/grafana:/var/lib/grafana \
          --link=influxdb:influxdb \
          --name grafana  grafana/grafana
      4. 打开localhost:5000来访问grafana的web服务
          login: admin  admin

36.docker------------zabbix  -------------------------------------
      docker pull busybox
      docker pull  monitoringartist/zabbix-xxl
      docker pull monitoringartist/zabbix-db-mariadb
      # create /var/lib/mysql as persistent volume storage
      docker run -d -v /var/lib/mysql --name zabbix-db-storage busybox:latest

      # start DB for Zabbix - default 1GB innodb_buffer_pool_size is used
      docker run \
            -d \
            --name zabbix-db \
            -v /backups:/backups \
            -v /etc/localtime:/etc/localtime:ro \
            --volumes-from zabbix-db-storage \
            --env="MARIADB_USER=zabbix" \
            --env="MARIADB_PASS=my_password" \
            monitoringartist/zabbix-db-mariadb
      # start Zabbix linked to started DB
            docker run \
            -d \
            --name zabbix \
            -p 80:80 \
            -p 10051:10051 \
            -v /etc/localtime:/etc/localtime:ro \
            --link zabbix-db:zabbix.db \
            --env="ZS_DBHost=zabbix.db" \
            --env="ZS_DBUser=zabbix" \
            --env="ZS_DBPassword=my_password" \
            monitoringartist/zabbix-xxl
        默认账号：Admin，密码：zabbix，这是一个超级管理员
      # Backup of DB Zabbix - configuration data only, no item history/trends
          docker exec \
          -ti zabbix-db \
          /zabbix-backup/zabbix-mariadb-dump -u zabbix -p my_password -o /backups
      # Full backup of Zabbix DB
          docker exec \
          -ti zabbix-db \
          bash -c "\
          mysqldump -u zabbix -pmy_password zabbix | \
          bzip2 -cq9 > /backups/zabbix_db_dump_$(date +%Y-%m-%d-%H.%M.%S).sql.bz2"
      # Restore Zabbix DB
      # remove zabbix server container
          docker rm -f zabbix
      # restore data from dump (all current data will be dropped!!!)
          docker exec -i zabbix-db sh -c 'bunzip2 -dc /backups/zabbix_db_dump_2016-05-25-02.57.46.sql.bz2 | mysql -uzabbix -p --password=my_password zabbix'
      # run zabbix server again
      --
 37. docker ---------------- grafana -----------------
      # create /var/lib/grafana as persistent volume storage
          docker run -d -v /var/lib/grafana --name grafana-xxl-storage busybox:latest
      # start grafana-xxl
          docker run \
              -d \
              -p 3000:3000 \
              --name grafana-xxl \
              -e UPGRADEALL=false \
              --volumes-from grafana-xxl-storage \
              monitoringartist/grafana-xxl:latest
      docker run -d --name=grafana-xxl -p 3000:3000 -e UPGRADEALL=false monitoringartist/grafana-xxl
      http://xxx.xxx.xxx.xxx:3000
      默认账号 admin，密码：admin，这是一个超级管理员

38.docker----------------prometheus- -------------------
      prometheus：
        tar xvfz prometheus-*.tar.gz
        cd prometheus-*
        prometheus.yml:
            global:
              scrape_interval:     15s # By default, scrape targets every 15 seconds.
              # Attach these labels to any time series or alerts when communicating with
              # external systems (federation, remote storage, Alertmanager).
              external_labels:
              monitor: 'codelab-monitor'
            # A scrape configuration containing exactly one endpoint to scrape:
            # Here it's Prometheus itself.
            scrape_configs:
              # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
              - job_name: 'prometheus'
              # Override the global default and scrape targets from this job every 5 seconds.
              scrape_interval: 5s
              static_configs:
                - targets: ['localhost:9090']
        # Start Prometheus.
        # By default, Prometheus stores its database in ./data (flag --storage.tsdb.path).
          ./prometheus --config.file=prometheus.yml
          docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \
                  prom/prometheus
        Or use an additional volume for the config:
        docker run -p 9090:9090 -v /prometheus-data \
              prom/prometheus --config.file=/prometheus-data/prometheus.yml
